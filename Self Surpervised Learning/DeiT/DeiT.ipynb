{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd68064d",
   "metadata": {},
   "source": [
    "# [ICLR 2021 Meta] Data-Efficient Image Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bd045",
   "metadata": {},
   "source": [
    "核心：改进ViT的**训练策略**，达到相对卷积有竞争力的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d9892f",
   "metadata": {},
   "source": [
    "- 只使用ImageNet\n",
    "- 纯血transformer，不包含任何卷积\n",
    "- DeiT 发现使用 Convnet 作为教师网络能够比使用 Transformer 架构效果更好"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b40b1",
   "metadata": {},
   "source": [
    "![alt text](<../../Note Image/Deit_overall.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40580f23",
   "metadata": {},
   "source": [
    "## Traning Strategy：**低分辨率训练，高分辨率微调，位置编码插值**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8334c9d4",
   "metadata": {},
   "source": [
    "##  Distillation Inovation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac9d116",
   "metadata": {},
   "source": [
    "![alt text](deit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3089d8c6",
   "metadata": {},
   "source": [
    "- 在末尾加上distillation token用来与teacher模型的输出计算蒸馏损失\n",
    "- distillation token很大程度上类似于class token:在transformer block中不断的和\n",
    "patch token,class token进行交互。不过class token的目标是跟真实的label一致，\n",
    "而distillation token是要跟teacher model预测的label一致。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
