{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《Emerging Properties in Self-Supervised Vision Transformers》——ICCV2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ctrl+Shift+V preview \n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Input Images] --> B[Multi-crop Augmentation]\n",
    "    B --> C[DINO Framework]\n",
    "    C --> D[Student Network]\n",
    "    C --> E[Teacher Network]\n",
    "    D --> F[Student Output]\n",
    "    E --> G[Teacher Output]\n",
    "    F --> H[DINO Loss]\n",
    "    G --> H\n",
    "    D --> I[EMA Update]\n",
    "    I --> E\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distillation with NO labels,用于在不需要标记数据的情况下训练ViT，本质上是一个Teacher-Student框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student-Teacher Framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../../Image/DINO_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../../Image/Student-Teacher.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Net——$g_{\\theta_s}$：学习预测教师网络的输出，通过梯度下降更新 <p>\n",
    "Teacher Net——$g_{\\theta_t}$：学习Target Representation，通过**学生网络参数EMA进行更新** <p>\n",
    "- 两个网络分别输出概率分布$P_s$和%$P_t$,这些概率分布是通过将两个网络的输出进行softmax而来的<p>\n",
    "$P_s(x)(i)=\\frac{\\exp(g_{\\theta_s}(x)(i)/\\tau_s)}{\\sum_{k=1}^K\\exp(g_{\\theta_s}(x)(k)/\\tau_s)}$;$\\tau_t$是温度参数，控制输出分布的锐度<p>\n",
    "- 交叉熵损失更新$\\theta_s$:$\\min_{\\theta_s}H(P_t(x),P_s(x))$ 其中,$H(a,b)=-a\\log b$\n",
    "- EMA更新教师网络：$\\theta_t=\\lambda \\theta_t+(1-\\lambda)\\theta_s$<p>\n",
    "其中，λ是一个接近1的值，通常在训练过程中从0.996线性衰减到1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOLoss(nn.Module):\n",
    "    def __init__(self, out_dim, ncrops, warmup_teacher_temp, teacher_temp,\n",
    "                 warmup_teacher_temp_epochs, nepochs, student_temp=0.1,\n",
    "                 center_momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.student_temp = student_temp\n",
    "        self.center_momentum = center_momentum\n",
    "        self.ncrops = ncrops\n",
    "        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n",
    "        # we apply a warm up for the teacher temperature because\n",
    "        # a too high temperature makes the training instable at the beginning\n",
    "        self.teacher_temp_schedule = np.concatenate((\n",
    "            np.linspace(warmup_teacher_temp,\n",
    "                        teacher_temp, warmup_teacher_temp_epochs),\n",
    "            np.ones(nepochs - warmup_teacher_temp_epochs) * teacher_temp\n",
    "        ))\n",
    "\n",
    "\n",
    "    def forward(self, student_output, teacher_output, epoch):\n",
    "        \"\"\"\n",
    "        Cross-entropy between softmax outputs of the teacher and student networks.\n",
    "        \"\"\"\n",
    "        student_out = student_output / self.student_temp\n",
    "        student_out = student_out.chunk(self.ncrops)\n",
    "\n",
    "\n",
    "        # teacher centering and sharpening\n",
    "        temp = self.teacher_temp_schedule[epoch]\n",
    "        teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1)\n",
    "        teacher_out = teacher_out.detach().chunk(2)\n",
    "\n",
    "\n",
    "        total_loss = 0\n",
    "        n_loss_terms = 0\n",
    "        for iq, q in enumerate(teacher_out):\n",
    "            for v in range(len(student_out)):\n",
    "                if v == iq:\n",
    "                    # we skip cases where student and teacher operate on the same view\n",
    "                    continue\n",
    "                loss = torch.sum(-q * F.log_softmax(student_out[v], dim=-1), dim=-1)\n",
    "                total_loss += loss.mean()\n",
    "                n_loss_terms += 1\n",
    "        total_loss /= n_loss_terms\n",
    "        self.update_center(teacher_output)\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_center(self, teacher_output):\n",
    "        \"\"\"\n",
    "        Update center used for teacher output.\n",
    "        \"\"\"\n",
    "        batch_center = torch.sum(teacher_output, dim=0, keepdim=True)\n",
    "        dist.all_reduce(batch_center)\n",
    "        batch_center = batch_center / (len(teacher_output) * dist.get_world_size())\n",
    "\n",
    "\n",
    "        # ema update\n",
    "        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**网络架构**：DINO的神经网络由一个主干网络（如ViT或ResNet）和一个投影头组成。<p>\n",
    "投影头是一个3层多层感知机（MLP），后面跟着一个权重归一化的全连接层，输出维度为K。<p>\n",
    "在训练过程中，我们不使用批量归一化（BN），因为ViT架构默认不使用BN。这种设计使得DINO在ViT上完全不依赖BN，提高了训练效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**避免坍塌**：为了避免模型坍塌 (即模型输出均匀分布或被单一维度主导),DINO采用了中心化和锐化操作。中心化操作通过更新中心向量$c$来避免单一维度主导，而锐化操作通过降低温度参数$\\tau_t$来避免均匀分布。具体公式如下：\n",
    "\n",
    "$$c\\leftarrow mc+(1-m)\\frac1B\\sum_{i=1}^Bg_{\\theta_t}(x_i)$$\n",
    "\n",
    "其中，$m$是更新率参数，$B$是批量大小。通过这两种操作，DINO能够在训练过程中保持稳定的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    # ============ building student and teacher networks ... ============\n",
    "    # we changed the name DeiT-S for ViT-S to avoid confusions\n",
    "    args.arch = args.arch.replace(\"deit\", \"vit\")\n",
    "\n",
    "    # 主要load：student model、teacher model and embed_dim\n",
    "    # if the network is a Vision Transformer (i.e. vit_tiny, vit_small, vit_base)\n",
    "    if args.arch in vits.__dict__.keys():\n",
    "        student = vits.__dict__[args.arch](\n",
    "            patch_size=args.patch_size,\n",
    "            drop_path_rate=args.drop_path_rate,  # stochastic depth\n",
    "        )\n",
    "        teacher = vits.__dict__[args.arch](patch_size=args.patch_size)\n",
    "        embed_dim = student.embed_dim\n",
    "\n",
    "    # if the network is a XCiT\n",
    "    elif args.arch in torch.hub.list(\"facebookresearch/xcit:main\"):\n",
    "        student = torch.hub.load('facebookresearch/xcit:main', args.arch,\n",
    "                                 pretrained=False, drop_path_rate=args.drop_path_rate)\n",
    "        teacher = torch.hub.load('facebookresearch/xcit:main', args.arch, pretrained=False)\n",
    "        embed_dim = student.embed_dim\n",
    "\n",
    "    # otherwise, we check if the architecture is in torchvision models\n",
    "    elif args.arch in torchvision_models.__dict__.keys():\n",
    "        student = torchvision_models.__dict__[args.arch]()\n",
    "        teacher = torchvision_models.__dict__[args.arch]()\n",
    "        embed_dim = student.fc.weight.shape[1]\n",
    "    else:\n",
    "        print(f\"Unknow architecture: {args.arch}\")\n",
    "\n",
    "    # multi-crop wrapper handles forward with inputs of different resolutions\n",
    "    student = utils.MultiCropWrapper(student, DINOHead(\n",
    "        embed_dim,\n",
    "        args.out_dim,\n",
    "        use_bn=args.use_bn_in_head,\n",
    "        norm_last_layer=args.norm_last_layer,\n",
    "    ))\n",
    "    teacher = utils.MultiCropWrapper(\n",
    "        teacher,\n",
    "        DINOHead(embed_dim, args.out_dim, args.use_bn_in_head),\n",
    "    )\n",
    "\n",
    "    # move networks to gpu\n",
    "    student, teacher = student.cuda(), teacher.cuda()\n",
    "\n",
    "    # synchronize batch norms (if any)\n",
    "    if utils.has_batchnorms(student):\n",
    "        student = nn.SyncBatchNorm.convert_sync_batchnorm(student)\n",
    "        teacher = nn.SyncBatchNorm.convert_sync_batchnorm(teacher)\n",
    "\n",
    "        # we need DDP wrapper to have synchro batch norms working...\n",
    "        teacher = nn.parallel.DistributedDataParallel(teacher, device_ids=[args.gpu])\n",
    "        teacher_without_ddp = teacher.module\n",
    "    else:\n",
    "        # teacher_without_ddp and teacher are the same thing\n",
    "        teacher_without_ddp = teacher\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    student = nn.parallel.DistributedDataParallel(student, device_ids=[args.gpu])\n",
    "    # teacher and student start with the same weights\n",
    "    teacher_without_ddp.load_state_dict(student.module.state_dict())\n",
    "    # there is no backpropagation through the teacher, so no need for gradients\n",
    "    for p in teacher.parameters():\n",
    "        p.requires_grad = False\n",
    "    # ---------------------------------------------------------------------------\n",
    "    print(f\"Student and Teacher are built: they are both {args.arch} network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Crop Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Glboal views:由两个网络处理的大规模图像（224×224 像素），由学生模型和教师模型共同处理\n",
    "- Local views:多个较小的裁剪（96×96 像素）仅由学生网络处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "global_crops_scale：全局裁剪的尺度范围 <p>\n",
    "local_crops_number：局部视图的数量 <P>\n",
    "local_crops_scale：局部裁剪的尺度范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ preparing data ... ============\n",
    "transform = DataAugmentationDINO(\n",
    "        args.global_crops_scale,\n",
    "        args.local_crops_scale,\n",
    "        args.local_crops_number,\n",
    "    )\n",
    "dataset = datasets.ImageFolder(args.data_path, transform=transform)\n",
    "sampler = torch.utils.data.DistributedSampler(dataset, shuffle=True)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        sampler=sampler,\n",
    "        batch_size=args.batch_size_per_gpu,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "print(f\"Data loaded: there are {len(dataset)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "class DataAugmentationDINO(object):\n",
    "    def __init__(self, global_crops_scale, local_crops_scale, local_crops_number):\n",
    "        flip_and_color_jitter = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "        ])\n",
    "        normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "\n",
    "        # first global crop\n",
    "        self.global_transfo1 = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n",
    "            flip_and_color_jitter,\n",
    "            utils.GaussianBlur(1.0),\n",
    "            normalize,\n",
    "        ])\n",
    "        # second global crop\n",
    "        self.global_transfo2 = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n",
    "            flip_and_color_jitter,\n",
    "            utils.GaussianBlur(0.1),\n",
    "            utils.Solarization(0.2),\n",
    "            normalize,\n",
    "        ])\n",
    "        # transformation for the local small crops\n",
    "        self.local_crops_number = local_crops_number\n",
    "        self.local_transfo = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(96, scale=local_crops_scale, interpolation=Image.BICUBIC),\n",
    "            flip_and_color_jitter,\n",
    "            utils.GaussianBlur(p=0.5),\n",
    "            normalize,\n",
    "        ])\n",
    "    def __call__(self, image):\n",
    "        crops = []\n",
    "        crops.append(self.global_transfo1(image))\n",
    "        crops.append(self.global_transfo2(image))\n",
    "        for _ in range(self.local_crops_number):\n",
    "            crops.append(self.local_transfo(image))\n",
    "        return crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 第一个全局裁剪\n",
    "    = 随机裁剪到224x224\n",
    "    - 应用翻转和颜色抖动\n",
    "    - 使用高斯模糊\n",
    "    - Normalize\n",
    "- 第二个全局视图\n",
    "    = 随机裁剪到224x224\n",
    "    - 应用翻转和颜色抖动\n",
    "    - 使用高斯模糊\n",
    "    - 应用 Solarization(曝光)\n",
    "    - Normalize\n",
    "- 局部视图：\n",
    "    - 将图像裁剪到更小的尺寸（96×96）\n",
    "    - 应用翻转和颜色抖动\n",
    "    - 使用高斯模糊（概率为0.5）\n",
    "    - 标准化\n",
    "\n",
    "通过这种方式进行多尺度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整体pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "        class_pos_embed = self.pos_embed[:, 0]\n",
    "        patch_pos_embed = self.pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size\n",
    "        h0 = h // self.patch_embed.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    def prepare_tokens(self, x):\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.patch_embed(x)  # patch linear embedding\n",
    "\n",
    "        # add the [CLS] token to the embed patch tokens\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def get_last_selfattention(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < len(self.blocks) - 1:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                # return attention of the last block\n",
    "                return blk(x, return_attention=True)\n",
    "\n",
    "    def get_intermediate_layers(self, x, n=1):\n",
    "        x = self.prepare_tokens(x)\n",
    "        # we return the output tokens from the `n` last blocks\n",
    "        output = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            if len(self.blocks) - i <= n:\n",
    "                output.append(self.norm(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forward(): 在处理完整个 transformer 后返回 [CLS] token 的表示, 用于分类和一般表示\n",
    "- get_last_selfattention(): 返回最后一个 transformer 模块的注意力图,用于注意力可视化与解释\n",
    "- get_intermediate_layers(): 返回最后 n 个 Transformer 模块的输出,用于细粒度特征提取和多尺度处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DINO haad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DINO 头是一个投影头，它将 ViT 表示转换为计算自监督损失的空间.它由以下部分组成：<p>\n",
    "一个将信息映射到瓶颈维度的多层感知器<p>\n",
    "特征的 L2 归一化<p>\n",
    "一个带有可选权重归一化的最终线性层\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "norm_las_layer提高训练稳定性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):\n",
    "        super().__init__()\n",
    "        nlayers = max(nlayers, 1)\n",
    "        if nlayers == 1:\n",
    "            self.mlp = nn.Linear(in_dim, bottleneck_dim)\n",
    "        else:\n",
    "            layers = [nn.Linear(in_dim, hidden_dim)]\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            for _ in range(nlayers - 2):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                if use_bn:\n",
    "                    layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "                layers.append(nn.GELU())\n",
    "            layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
    "            self.mlp = nn.Sequential(*layers)\n",
    "        self.apply(self._init_weights)\n",
    "        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))\n",
    "        self.last_layer.weight_g.data.fill_(1)\n",
    "        if norm_last_layer:\n",
    "            self.last_layer.weight_g.requires_grad = False\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        x = nn.functional.normalize(x, dim=-1, p=2)\n",
    "        x = self.last_layer(x)\n",
    "        return x\n",
    "# -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DINO Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heavy_daily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
