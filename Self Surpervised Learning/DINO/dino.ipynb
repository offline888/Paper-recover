{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《Emerging Properties in Self-Supervised Vision Transformers》——ICCV2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ctrl+Shift+V preview \n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Input Images] --> B[Multi-crop Augmentation]\n",
    "    B --> C[DINO Framework]\n",
    "    C --> D[Student Network]\n",
    "    C --> E[Teacher Network]\n",
    "    D --> F[Student Output]\n",
    "    E --> G[Teacher Output]\n",
    "    F --> H[DINO Loss]\n",
    "    G --> H\n",
    "    D --> I[EMA Update]\n",
    "    I --> E\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distillation with NO labels,用于在不需要标记数据的情况下训练ViT，本质上是一个Teacher-Student框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student-Teacher Framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../../Image/DINO_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../../Image/Student-Teacher.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Net——$g_{\\theta_s}$：学习预测教师网络的输出，通过梯度下降更新 <p>\n",
    "Teacher Net——$g_{\\theta_t}$：学习Target Representation，通过**学生网络参数EMA进行更新** <p>\n",
    "- 两个网络分别输出概率分布$P_s$和%$P_t$,这些概率分布是通过将两个网络的输出进行softmax而来的<p>\n",
    "$P_s(x)(i)=\\frac{\\exp(g_{\\theta_s}(x)(i)/\\tau_s)}{\\sum_{k=1}^K\\exp(g_{\\theta_s}(x)(k)/\\tau_s)}$;$\\tau_t$是温度参数，控制输出分布的锐度<p>\n",
    "- 交叉熵损失更新$\\theta_s$:$\\min_{\\theta_s}H(P_t(x),P_s(x))$ 其中,$H(a,b)=-a\\log b$\n",
    "- EMA更新教师网络：$\\theta_t=\\lambda \\theta_t+(1-\\lambda)\\theta_s$<p>\n",
    "其中，λ是一个接近1的值，通常在训练过程中从0.996线性衰减到1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**网络架构**：DINO的神经网络由一个主干网络（如ViT或ResNet）和一个投影头组成。<p>\n",
    "投影头是一个3层多层感知机（MLP），后面跟着一个权重归一化的全连接层，输出维度为K。在训练过程中，我们不使用批量归一化（BN），因为ViT架构默认不使用BN。这种设计使得DINO在ViT上完全不依赖BN，提高了训练效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    # ============ building student and teacher networks ... ============\n",
    "    # we changed the name DeiT-S for ViT-S to avoid confusions\n",
    "    args.arch = args.arch.replace(\"deit\", \"vit\")\n",
    "\n",
    "    # 主要load：student model、teacher model and embed_dim\n",
    "    # if the network is a Vision Transformer (i.e. vit_tiny, vit_small, vit_base)\n",
    "    if args.arch in vits.__dict__.keys():\n",
    "        student = vits.__dict__[args.arch](\n",
    "            patch_size=args.patch_size,\n",
    "            drop_path_rate=args.drop_path_rate,  # stochastic depth\n",
    "        )\n",
    "        teacher = vits.__dict__[args.arch](patch_size=args.patch_size)\n",
    "        embed_dim = student.embed_dim\n",
    "\n",
    "    # if the network is a XCiT\n",
    "    elif args.arch in torch.hub.list(\"facebookresearch/xcit:main\"):\n",
    "        student = torch.hub.load('facebookresearch/xcit:main', args.arch,\n",
    "                                 pretrained=False, drop_path_rate=args.drop_path_rate)\n",
    "        teacher = torch.hub.load('facebookresearch/xcit:main', args.arch, pretrained=False)\n",
    "        embed_dim = student.embed_dim\n",
    "\n",
    "    # otherwise, we check if the architecture is in torchvision models\n",
    "    elif args.arch in torchvision_models.__dict__.keys():\n",
    "        student = torchvision_models.__dict__[args.arch]()\n",
    "        teacher = torchvision_models.__dict__[args.arch]()\n",
    "        embed_dim = student.fc.weight.shape[1]\n",
    "    else:\n",
    "        print(f\"Unknow architecture: {args.arch}\")\n",
    "\n",
    "    # multi-crop wrapper handles forward with inputs of different resolutions\n",
    "    student = utils.MultiCropWrapper(student, DINOHead(\n",
    "        embed_dim,\n",
    "        args.out_dim,\n",
    "        use_bn=args.use_bn_in_head,\n",
    "        norm_last_layer=args.norm_last_layer,\n",
    "    ))\n",
    "    teacher = utils.MultiCropWrapper(\n",
    "        teacher,\n",
    "        DINOHead(embed_dim, args.out_dim, args.use_bn_in_head),\n",
    "    )\n",
    "\n",
    "    # move networks to gpu\n",
    "    student, teacher = student.cuda(), teacher.cuda()\n",
    "\n",
    "    # synchronize batch norms (if any)\n",
    "    if utils.has_batchnorms(student):\n",
    "        student = nn.SyncBatchNorm.convert_sync_batchnorm(student)\n",
    "        teacher = nn.SyncBatchNorm.convert_sync_batchnorm(teacher)\n",
    "\n",
    "        # we need DDP wrapper to have synchro batch norms working...\n",
    "        teacher = nn.parallel.DistributedDataParallel(teacher, device_ids=[args.gpu])\n",
    "        teacher_without_ddp = teacher.module\n",
    "    else:\n",
    "        # teacher_without_ddp and teacher are the same thing\n",
    "        teacher_without_ddp = teacher\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    student = nn.parallel.DistributedDataParallel(student, device_ids=[args.gpu])\n",
    "    # teacher and student start with the same weights\n",
    "    teacher_without_ddp.load_state_dict(student.module.state_dict())\n",
    "    # there is no backpropagation through the teacher, so no need for gradients\n",
    "    for p in teacher.parameters():\n",
    "        p.requires_grad = False\n",
    "    # ---------------------------------------------------------------------------\n",
    "    print(f\"Student and Teacher are built: they are both {args.arch} network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Crop Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Glboal views:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heavy_daily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
