{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《Emerging Properties in Self-Supervised Vision Transformers》——ICCV2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[Input Images] --> B[Multi-crop Augmentation]\n",
    "    B --> C[DINO Framework]\n",
    "    C --> D[Student Network]\n",
    "    C --> E[Teacher Network]\n",
    "    D --> F[Student Output]\n",
    "    E --> G[Teacher Output]\n",
    "    F --> H[DINO Loss]\n",
    "    G --> H\n",
    "    D --> I[EMA Update]\n",
    "    I --> E\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distillation with NO labels,用于在不需要标记数据的情况下训练ViT，本质上是一个Teacher-Student框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student-Teacher Framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Net——$g_{\\theta_s}$：学习预测教师网络的输出，通过梯度下降更新 <p>\n",
    "Teacher Net——$g_{\\theta_t}$：学习Target Representation，通过**学生网络参数EMA进行更新** <p>\n",
    "- 两个网络分别输出概率分布$P_s$和%$P_t$,这些概率分布是通过将两个网络的输出进行softmax而来的<p>\n",
    "$P_s(x)(i)=\\frac{\\exp(g_{\\theta_s}(x)(i)/\\tau_s)}{\\sum_{k=1}^K\\exp(g_{\\theta_s}(x)(k)/\\tau_s)}$;$\\tau_t$是温度参数，控制输出分布的锐度<p>\n",
    "- 交叉熵损失更新$\\theta_s$:$\\min_{\\theta_s}H(P_t(x),P_s(x))$ 其中,$H(a,b)=-a\\log b$\n",
    "- EMA更新教师网络：$\\theta_t=\\lambda \\theta_t+(1-\\lambda)\\theta_s$<p>\n",
    "其中，λ是一个接近1的值，通常在训练过程中从0.996线性衰减到1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOLoss(nn.Module):\n",
    "    def __init__(self, out_dim, ncrops, warmup_teacher_temp, teacher_temp,\n",
    "                 warmup_teacher_temp_epochs, nepochs, student_temp=0.1,\n",
    "                 center_momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.student_temp = student_temp\n",
    "        self.center_momentum = center_momentum\n",
    "        self.ncrops = ncrops\n",
    "        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n",
    "        # we apply a warm up for the teacher temperature because\n",
    "        # a too high temperature makes the training instable at the beginning\n",
    "        self.teacher_temp_schedule = np.concatenate((\n",
    "            np.linspace(warmup_teacher_temp,\n",
    "                        teacher_temp, warmup_teacher_temp_epochs),\n",
    "            np.ones(nepochs - warmup_teacher_temp_epochs) * teacher_temp\n",
    "        ))\n",
    "\n",
    "\n",
    "    def forward(self, student_output, teacher_output, epoch):\n",
    "        \"\"\"\n",
    "        Cross-entropy between softmax outputs of the teacher and student networks.\n",
    "        \"\"\"\n",
    "        student_out = student_output / self.student_temp\n",
    "        student_out = student_out.chunk(self.ncrops)\n",
    "\n",
    "\n",
    "        # teacher centering and sharpening\n",
    "        temp = self.teacher_temp_schedule[epoch]\n",
    "        teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1)\n",
    "        teacher_out = teacher_out.detach().chunk(2)\n",
    "\n",
    "\n",
    "        total_loss = 0\n",
    "        n_loss_terms = 0\n",
    "        for iq, q in enumerate(teacher_out):\n",
    "            for v in range(len(student_out)):\n",
    "                if v == iq:\n",
    "                    # we skip cases where student and teacher operate on the same view\n",
    "                    continue\n",
    "                loss = torch.sum(-q * F.log_softmax(student_out[v], dim=-1), dim=-1)\n",
    "                total_loss += loss.mean()\n",
    "                n_loss_terms += 1\n",
    "        total_loss /= n_loss_terms\n",
    "        self.update_center(teacher_output)\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_center(self, teacher_output):\n",
    "        \"\"\"\n",
    "        Update center used for teacher output.\n",
    "        \"\"\"\n",
    "        batch_center = torch.sum(teacher_output, dim=0, keepdim=True)\n",
    "        dist.all_reduce(batch_center)\n",
    "        batch_center = batch_center / (len(teacher_output) * dist.get_world_size())\n",
    "\n",
    "\n",
    "        # ema update\n",
    "        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**网络架构**：DINO的神经网络由一个主干网络（如ViT或ResNet）和一个投影头组成。<p>\n",
    "投影头是一个3层多层感知机（MLP），后面跟着一个权重归一化的全连接层，输出维度为K。<p>\n",
    "在训练过程中，我们不使用批量归一化（BN），因为ViT架构默认不使用BN。这种设计使得DINO在ViT上完全不依赖BN，提高了训练效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**避免坍塌**：为了避免模型坍塌 (即模型输出均匀分布或被单一维度主导),DINO采用了中心化和锐化操作。中心化操作通过更新中心向量$c$来避免单一维度主导，而锐化操作通过降低温度参数$\\tau_t$来避免均匀分布。具体公式如下：\n",
    "\n",
    "$$c\\leftarrow mc+(1-m)\\frac1B\\sum_{i=1}^Bg_{\\theta_t}(x_i)$$\n",
    "\n",
    "其中，$m$是更新率参数，$B$是批量大小。通过这两种操作，DINO能够在训练过程中保持稳定的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    # ============ building student and teacher networks ... ============\n",
    "    # we changed the name DeiT-S for ViT-S to avoid confusions\n",
    "    args.arch = args.arch.replace(\"deit\", \"vit\")\n",
    "\n",
    "    # 主要load：student model、teacher model and embed_dim\n",
    "    # if the network is a Vision Transformer (i.e. vit_tiny, vit_small, vit_base)\n",
    "    if args.arch in vits.__dict__.keys():\n",
    "        student = vits.__dict__[args.arch](\n",
    "            patch_size=args.patch_size,\n",
    "            drop_path_rate=args.drop_path_rate,  # stochastic depth\n",
    "        )\n",
    "        teacher = vits.__dict__[args.arch](patch_size=args.patch_size)\n",
    "        embed_dim = student.embed_dim\n",
    "\n",
    "    # if the network is a XCiT\n",
    "    elif args.arch in torch.hub.list(\"facebookresearch/xcit:main\"):\n",
    "        student = torch.hub.load('facebookresearch/xcit:main', args.arch,\n",
    "                                 pretrained=False, drop_path_rate=args.drop_path_rate)\n",
    "        teacher = torch.hub.load('facebookresearch/xcit:main', args.arch, pretrained=False)\n",
    "        embed_dim = student.embed_dim\n",
    "\n",
    "    # otherwise, we check if the architecture is in torchvision models\n",
    "    elif args.arch in torchvision_models.__dict__.keys():\n",
    "        student = torchvision_models.__dict__[args.arch]()\n",
    "        teacher = torchvision_models.__dict__[args.arch]()\n",
    "        embed_dim = student.fc.weight.shape[1]\n",
    "    else:\n",
    "        print(f\"Unknow architecture: {args.arch}\")\n",
    "\n",
    "    # multi-crop wrapper handles forward with inputs of different resolutions\n",
    "    student = utils.MultiCropWrapper(student, DINOHead(\n",
    "        embed_dim,\n",
    "        args.out_dim,\n",
    "        use_bn=args.use_bn_in_head,\n",
    "        norm_last_layer=args.norm_last_layer,\n",
    "    ))\n",
    "    teacher = utils.MultiCropWrapper(\n",
    "        teacher,\n",
    "        DINOHead(embed_dim, args.out_dim, args.use_bn_in_head),\n",
    "    )\n",
    "\n",
    "    # move networks to gpu\n",
    "    student, teacher = student.cuda(), teacher.cuda()\n",
    "\n",
    "    # synchronize batch norms (if any)\n",
    "    if utils.has_batchnorms(student):\n",
    "        student = nn.SyncBatchNorm.convert_sync_batchnorm(student)\n",
    "        teacher = nn.SyncBatchNorm.convert_sync_batchnorm(teacher)\n",
    "\n",
    "        # we need DDP wrapper to have synchro batch norms working...\n",
    "        teacher = nn.parallel.DistributedDataParallel(teacher, device_ids=[args.gpu])\n",
    "        teacher_without_ddp = teacher.module\n",
    "    else:\n",
    "        # teacher_without_ddp and teacher are the same thing\n",
    "        teacher_without_ddp = teacher\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    student = nn.parallel.DistributedDataParallel(student, device_ids=[args.gpu])\n",
    "    # teacher and student start with the same weights\n",
    "    teacher_without_ddp.load_state_dict(student.module.state_dict())\n",
    "    # there is no backpropagation through the teacher, so no need for gradients\n",
    "    for p in teacher.parameters():\n",
    "        p.requires_grad = False\n",
    "    # ---------------------------------------------------------------------------\n",
    "    print(f\"Student and Teacher are built: they are both {args.arch} network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Crop Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Glboal views:由两个网络处理的大规模图像（224×224 像素），由学生模型和教师模型共同处理\n",
    "- Local views:多个较小的裁剪（96×96 像素）仅由学生网络处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "global_crops_scale：全局裁剪的尺度范围 <p>\n",
    "local_crops_number：局部视图的数量 <P>\n",
    "local_crops_scale：局部裁剪的尺度范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ preparing data ... ============\n",
    "transform = DataAugmentationDINO(\n",
    "        args.global_crops_scale,\n",
    "        args.local_crops_scale,\n",
    "        args.local_crops_number,\n",
    "    )\n",
    "dataset = datasets.ImageFolder(args.data_path, transform=transform)\n",
    "sampler = torch.utils.data.DistributedSampler(dataset, shuffle=True)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        sampler=sampler,\n",
    "        batch_size=args.batch_size_per_gpu,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "print(f\"Data loaded: there are {len(dataset)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "class DataAugmentationDINO(object):\n",
    "    def __init__(self, global_crops_scale, local_crops_scale, local_crops_number):\n",
    "        flip_and_color_jitter = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "        ])\n",
    "        normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "\n",
    "        # first global crop\n",
    "        self.global_transfo1 = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n",
    "            flip_and_color_jitter,\n",
    "            utils.GaussianBlur(1.0),\n",
    "            normalize,\n",
    "        ])\n",
    "        # second global crop\n",
    "        self.global_transfo2 = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n",
    "            flip_and_color_jitter,\n",
    "            utils.GaussianBlur(0.1),\n",
    "            utils.Solarization(0.2),\n",
    "            normalize,\n",
    "        ])\n",
    "        # transformation for the local small crops\n",
    "        self.local_crops_number = local_crops_number\n",
    "        self.local_transfo = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(96, scale=local_crops_scale, interpolation=Image.BICUBIC),\n",
    "            flip_and_color_jitter,\n",
    "            utils.GaussianBlur(p=0.5),\n",
    "            normalize,\n",
    "        ])\n",
    "    def __call__(self, image):\n",
    "        crops = []\n",
    "        crops.append(self.global_transfo1(image))\n",
    "        crops.append(self.global_transfo2(image))\n",
    "        for _ in range(self.local_crops_number):\n",
    "            crops.append(self.local_transfo(image))\n",
    "        return crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 第一个全局裁剪\n",
    "    = 随机裁剪到224x224\n",
    "    - 应用翻转和颜色抖动\n",
    "    - 使用高斯模糊\n",
    "    - Normalize\n",
    "- 第二个全局视图\n",
    "    = 随机裁剪到224x224\n",
    "    - 应用翻转和颜色抖动\n",
    "    - 使用高斯模糊\n",
    "    - 应用 Solarization(曝光)\n",
    "    - Normalize\n",
    "- 局部视图：\n",
    "    - 将图像裁剪到更小的尺寸（96×96）\n",
    "    - 应用翻转和颜色抖动\n",
    "    - 使用高斯模糊（概率为0.5）\n",
    "    - 标准化\n",
    "\n",
    "通过这种方式进行多尺度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT Block：Patch embedding+ViTBlock+DINO Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整体pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "        class_pos_embed = self.pos_embed[:, 0]\n",
    "        patch_pos_embed = self.pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size\n",
    "        h0 = h // self.patch_embed.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    def prepare_tokens(self, x):\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.patch_embed(x)  # patch linear embedding\n",
    "\n",
    "        # add the [CLS] token to the embed patch tokens\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def get_last_selfattention(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < len(self.blocks) - 1:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                # return attention of the last block\n",
    "                return blk(x, return_attention=True)\n",
    "\n",
    "    def get_intermediate_layers(self, x, n=1):\n",
    "        x = self.prepare_tokens(x)\n",
    "        # we return the output tokens from the `n` last blocks\n",
    "        output = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            if len(self.blocks) - i <= n:\n",
    "                output.append(self.norm(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forward(): 在处理完整个 transformer 后返回 [CLS] token 的表示, 用于分类和一般表示\n",
    "- get_last_selfattention(): 返回最后一个 transformer 模块的注意力图,用于注意力可视化与解释\n",
    "- get_intermediate_layers(): 返回最后 n 个 Transformer 模块的输出,用于细粒度特征提取和多尺度处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DINO Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "norm_las_layer提高训练稳定性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):\n",
    "        super().__init__()\n",
    "        nlayers = max(nlayers, 1)\n",
    "        if nlayers == 1:\n",
    "            self.mlp = nn.Linear(in_dim, bottleneck_dim)\n",
    "        else:\n",
    "            layers = [nn.Linear(in_dim, hidden_dim)]\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            for _ in range(nlayers - 2):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                if use_bn:\n",
    "                    layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "                layers.append(nn.GELU())\n",
    "            layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
    "            self.mlp = nn.Sequential(*layers)\n",
    "        self.apply(self._init_weights)\n",
    "        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))\n",
    "        self.last_layer.weight_g.data.fill_(1)\n",
    "        if norm_last_layer:\n",
    "            self.last_layer.weight_g.requires_grad = False\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        x = nn.functional.normalize(x, dim=-1, p=2)\n",
    "        x = self.last_layer(x)\n",
    "        return x\n",
    "# -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DINO Training：单节点训练和多节点训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval：评估DINO学到的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.分布式通信版本的特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "@torch.no_grad()\n",
    "def extract_features(model, data_loader, use_cuda=True, multiscale=False):\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    features = None\n",
    "    for samples, index in metric_logger.log_every(data_loader, 10):\n",
    "        samples = samples.cuda(non_blocking=True)\n",
    "        index = index.cuda(non_blocking=True)\n",
    "        # 使用多尺度视图进行特征提取\n",
    "        if multiscale:\n",
    "            feats = utils.multi_scale(samples, model)\n",
    "        else:\n",
    "            feats = model(samples).clone()\n",
    "\n",
    "        # init storage feature matrix，特征存储矩阵的初始化\n",
    "        if dist.get_rank() == 0 and features is None:\n",
    "            features = torch.zeros(len(data_loader.dataset), feats.shape[-1])\n",
    "            if use_cuda:\n",
    "                features = features.cuda(non_blocking=True)\n",
    "            print(f\"Storing features into tensor of shape {features.shape}\")\n",
    "\n",
    "        # get indexes from all processes\n",
    "        # 使用all_gather收集所有GPU上的特征\n",
    "        # 使用index_copy_将特征放入正确的位置\n",
    "        y_all = torch.empty(dist.get_world_size(), index.size(0), dtype=index.dtype, device=index.device)\n",
    "        y_l = list(y_all.unbind(0))\n",
    "        y_all_reduce = torch.distributed.all_gather(y_l, index, async_op=True)\n",
    "        y_all_reduce.wait()\n",
    "        index_all = torch.cat(y_l)\n",
    "\n",
    "        # share features between processes\n",
    "        feats_all = torch.empty(\n",
    "            dist.get_world_size(),\n",
    "            feats.size(0),\n",
    "            feats.size(1),\n",
    "            dtype=feats.dtype,\n",
    "            device=feats.device,\n",
    "        )\n",
    "        output_l = list(feats_all.unbind(0))\n",
    "        output_all_reduce = torch.distributed.all_gather(output_l, feats, async_op=True)\n",
    "        output_all_reduce.wait()\n",
    "\n",
    "        # update storage feature matrix\n",
    "        if dist.get_rank() == 0:\n",
    "            if use_cuda:\n",
    "                features.index_copy_(0, index_all, torch.cat(output_l))\n",
    "            else:\n",
    "                features.index_copy_(0, index_all.cpu(), torch.cat(output_l).cpu())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.K-NN分类器\n",
    "使用预训练模型，无需进行额外训练参数\n",
    "```\n",
    "- 1.Extracts features from both training and validation images using the pretrained model\n",
    "- 2.Computes similarity between validation features and all training features\n",
    "- 3.For each validation image, finds k nearest neighbors in the training set\n",
    "- 4.Makes predictions based on weighted voting from the k nearest neighbors\n",
    "- 5.Reports Top-1 and Top-5 accuracy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def knn_classifier(train_features, train_labels, test_features, test_labels, k, T, num_classes=1000):\n",
    "    top1, top5, total = 0.0, 0.0, 0\n",
    "    train_features = train_features.t()\n",
    "    num_test_images, num_chunks = test_labels.shape[0], 100\n",
    "    imgs_per_chunk = num_test_images // num_chunks\n",
    "    retrieval_one_hot = torch.zeros(k, num_classes).to(train_features.device)\n",
    "    for idx in range(0, num_test_images, imgs_per_chunk):\n",
    "        # get the features for test images\n",
    "        features = test_features[\n",
    "            idx : min((idx + imgs_per_chunk), num_test_images), :\n",
    "        ]\n",
    "        targets = test_labels[idx : min((idx + imgs_per_chunk), num_test_images)]\n",
    "        batch_size = targets.shape[0]\n",
    "\n",
    "\n",
    "        # calculate the dot product and compute top-k neighbors\n",
    "        # --------------------------------------------------------------------------------\n",
    "        # 计算相似度\n",
    "        similarity = torch.mm(features, train_features)\n",
    "        distances, indices = similarity.topk(k, largest=True, sorted=True)\n",
    "        # 获取k个最近邻的标签\n",
    "        candidates = train_labels.view(1, -1).expand(batch_size, -1)\n",
    "        retrieved_neighbors = torch.gather(candidates, 1, indices)\n",
    "        # ---------------------------------------------------------------------------------\n",
    "\n",
    "        retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()\n",
    "        retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)\n",
    "        distances_transform = distances.clone().div_(T).exp_()\n",
    "        probs = torch.sum(\n",
    "            torch.mul(\n",
    "                retrieval_one_hot.view(batch_size, -1, num_classes),\n",
    "                distances_transform.view(batch_size, -1, 1),\n",
    "            ),\n",
    "            1,\n",
    "        )\n",
    "        _, predictions = probs.sort(1, True)\n",
    "\n",
    "\n",
    "        # find the predictions that match the target\n",
    "        correct = predictions.eq(targets.data.view(-1, 1))\n",
    "        top1 = top1 + correct.narrow(1, 0, 1).sum().item()\n",
    "        top5 = top5 + correct.narrow(1, 0, min(5, k)).sum().item()  # top5 does not make sense if k < 5\n",
    "        total += targets.size(0)\n",
    "    top1 = top1 * 100.0 / total\n",
    "    top5 = top5 * 100.0 / total\n",
    "    return top1, top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Linear Evaluation\n",
    "线性评估在预训练模型的冻结特征上训练一个线性分类器，为表征质量提供更具判别性的度量。<p>\n",
    "- 1.使用冻结的预训练主干从图像中提取特征\n",
    "- 2.对于 ViT 模型：\n",
    "    -  从最后 n 个块中获取中间输出\n",
    "    -  连接[CLS]标记特征\n",
    "    -  可选地添加平均池化块标记特征\n",
    "- 3.在这些特征之上训练一个简单的线性分类器<p>\n",
    "- 4.在验证集上报告 Top-1 和 Top-5 准确率<p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eval_linear`:主评估函数\n",
    "`train`:训练函数\n",
    "`validate`:验证函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_linear核心\n",
    "# 1. 模型构建和初始化\n",
    "if args.arch in vits.__dict__.keys():\n",
    "    model = vits.__dict__[args.arch](patch_size=args.patch_size, num_classes=0)\n",
    "    embed_dim = model.embed_dim * (args.n_last_blocks + int(args.avgpool_patchtokens))\n",
    "\n",
    "# 2. 线性分类器初始化\n",
    "linear_classifier = LinearClassifier(embed_dim, num_labels=args.num_labels)\n",
    "\n",
    "# 3. 训练循环\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    train_stats = train(model, linear_classifier, optimizer, train_loader, epoch, args.n_last_blocks, args.avgpool_patchtokens)\n",
    "    if epoch % args.val_freq == 0:\n",
    "        test_stats = validate_network(val_loader, model, linear_classifier, args.n_last_blocks, args.avgpool_patchtokens)\n",
    "        best_acc = max(best_acc, test_stats[\"acc1\"])\n",
    "\n",
    "\n",
    "# train核心\n",
    "# 1. 特征提取\n",
    "with torch.no_grad():\n",
    "    if \"vit\" in args.arch:\n",
    "        intermediate_output = model.get_intermediate_layers(inp, n)\n",
    "        output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "        if avgpool:\n",
    "            output = torch.cat((output.unsqueeze(-1), torch.mean(intermediate_output[-1][:, 1:], dim=1).unsqueeze(-1)), dim=-1)\n",
    "            output = output.reshape(output.shape[0], -1)\n",
    "    else:\n",
    "        output = model(inp)\n",
    "\n",
    "# 2. 分类和损失计算\n",
    "output = linear_classifier(output)\n",
    "loss = nn.CrossEntropyLoss()(output, target)\n",
    "\n",
    "# 3. 反向传播和优化\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "#validate_network核心\n",
    "# 1. 特征提取（与train函数相同）\n",
    "with torch.no_grad():\n",
    "    if \"vit\" in args.arch:\n",
    "        intermediate_output = model.get_intermediate_layers(inp, n)\n",
    "        output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "        if avgpool:\n",
    "            output = torch.cat((output.unsqueeze(-1), torch.mean(intermediate_output[-1][:, 1:], dim=1).unsqueeze(-1)), dim=-1)\n",
    "            output = output.reshape(output.shape[0], -1)\n",
    "    else:\n",
    "        output = model(inp)\n",
    "\n",
    "# 2. 分类和准确率计算\n",
    "output = linear_classifier(output)\n",
    "if linear_classifier.module.num_labels >= 5:\n",
    "    acc1, acc5 = utils.accuracy(output, target, topk=(1, 5))\n",
    "else:\n",
    "    acc1, = utils.accuracy(output, target, topk=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Image Retrieval & Copy Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.Video Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "1. 静态注意力可视化:生成热图，显示单个图像的注意力模式<p>\n",
    "    - 从 Vision Transformer 最后一层的 [CLS] 标记中提取自注意力图<p>\n",
    "2. 视频注意力可视化:创建带有注意力叠加的视频，以可视化视频帧中的注意力模式"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heavy_daily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
