{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c03d882",
   "metadata": {},
   "source": [
    "# IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Model \n",
    "\n",
    "**核心**:修改cross attn,在原本text分支上引入image分支\n",
    "- 提供reference,训练映射层和cross attn层，和原本的text cross attn的输出拼接，引导图像生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7c574",
   "metadata": {},
   "source": [
    "![alt text](../Image/IP-adapter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae458d",
   "metadata": {},
   "source": [
    "Variant | Base Class | Projection Model | Image Embedding Source | Compatible With | Special Characteristics\n",
    "--- | --- | --- | --- | --- | ---\n",
    "IPAdapter | - | ImageProjModel | CLIP final embeddings | SD 1.5 | Base implementation\n",
    "IPAdapterPlus | IPAdapter | Resampler | CLIP hidden states[-2] | SD 1.5 | Fine-grained features\n",
    "IPAdapterXL | IPAdapter | ImageProjModel | CLIP final embeddings | SDXL | SDXL-specific generation\n",
    "IPAdapterFull | IPAdapterPlus | MLPProjModel | CLIP hidden states[-2] | SD 1.5 | MLP-based projection\n",
    "IPAdapterPlusXL | IPAdapter | Resampler (XL dims) | CLIP hidden states[-2] | SDXL | Fine-grained features + SDXL compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02add59",
   "metadata": {},
   "source": [
    "代码核心实现:IP-Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c77c9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class IPAdapter:\n",
    "    def __init__(self, sd_pipe, image_encoder_path, ip_ckpt, device, num_tokens=4):\n",
    "        self.device = device\n",
    "        self.image_encoder_path = image_encoder_path\n",
    "        self.ip_ckpt = ip_ckpt\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "        self.pipe = sd_pipe.to(self.device)\n",
    "        self.set_ip_adapter()\n",
    "\n",
    "        # load image encoder\n",
    "        self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(self.image_encoder_path).to(\n",
    "            self.device, dtype=torch.float16\n",
    "        )\n",
    "        self.clip_image_processor = CLIPImageProcessor()\n",
    "        # image proj model\n",
    "        self.image_proj_model = self.init_proj()\n",
    "\n",
    "        self.load_ip_adapter()\n",
    "\n",
    "    def init_proj(self):\n",
    "        image_proj_model = ImageProjModel(\n",
    "            cross_attention_dim=self.pipe.unet.config.cross_attention_dim,\n",
    "            clip_embeddings_dim=self.image_encoder.config.projection_dim,\n",
    "            clip_extra_context_tokens=self.num_tokens,\n",
    "        ).to(self.device, dtype=torch.float16)\n",
    "        return image_proj_model\n",
    "\n",
    "    def set_ip_adapter(self):\n",
    "        unet = self.pipe.unet\n",
    "        attn_procs = {}\n",
    "        for name in unet.attn_processors.keys():\n",
    "            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "            if name.startswith(\"mid_block\"):\n",
    "                hidden_size = unet.config.block_out_channels[-1]\n",
    "            elif name.startswith(\"up_blocks\"):\n",
    "                block_id = int(name[len(\"up_blocks.\")])\n",
    "                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "            elif name.startswith(\"down_blocks\"):\n",
    "                block_id = int(name[len(\"down_blocks.\")])\n",
    "                hidden_size = unet.config.block_out_channels[block_id]\n",
    "            if cross_attention_dim is None:\n",
    "                attn_procs[name] = AttnProcessor()\n",
    "            else:\n",
    "                attn_procs[name] = IPAttnProcessor(\n",
    "                    hidden_size=hidden_size,\n",
    "                    cross_attention_dim=cross_attention_dim,\n",
    "                    scale=1.0,\n",
    "                    num_tokens=self.num_tokens,\n",
    "                ).to(self.device, dtype=torch.float16)\n",
    "        unet.set_attn_processor(attn_procs)\n",
    "        if hasattr(self.pipe, \"controlnet\"):\n",
    "            if isinstance(self.pipe.controlnet, MultiControlNetModel):\n",
    "                for controlnet in self.pipe.controlnet.nets:\n",
    "                    controlnet.set_attn_processor(CNAttnProcessor(num_tokens=self.num_tokens))\n",
    "            else:\n",
    "                self.pipe.controlnet.set_attn_processor(CNAttnProcessor(num_tokens=self.num_tokens))\n",
    "\n",
    "    def load_ip_adapter(self):\n",
    "        if os.path.splitext(self.ip_ckpt)[-1] == \".safetensors\":\n",
    "            state_dict = {\"image_proj\": {}, \"ip_adapter\": {}}\n",
    "            with safe_open(self.ip_ckpt, framework=\"pt\", device=\"cpu\") as f:\n",
    "                for key in f.keys():\n",
    "                    if key.startswith(\"image_proj.\"):\n",
    "                        state_dict[\"image_proj\"][key.replace(\"image_proj.\", \"\")] = f.get_tensor(key)\n",
    "                    elif key.startswith(\"ip_adapter.\"):\n",
    "                        state_dict[\"ip_adapter\"][key.replace(\"ip_adapter.\", \"\")] = f.get_tensor(key)\n",
    "        else:\n",
    "            state_dict = torch.load(self.ip_ckpt, map_location=\"cpu\")\n",
    "        self.image_proj_model.load_state_dict(state_dict[\"image_proj\"])\n",
    "        ip_layers = torch.nn.ModuleList(self.pipe.unet.attn_processors.values())\n",
    "        ip_layers.load_state_dict(state_dict[\"ip_adapter\"])\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def get_image_embeds(self, pil_image=None, clip_image_embeds=None):\n",
    "        if pil_image is not None:\n",
    "            if isinstance(pil_image, Image.Image):\n",
    "                pil_image = [pil_image]\n",
    "            clip_image = self.clip_image_processor(images=pil_image, return_tensors=\"pt\").pixel_values\n",
    "            clip_image_embeds = self.image_encoder(clip_image.to(self.device, dtype=torch.float16)).image_embeds\n",
    "        else:\n",
    "            clip_image_embeds = clip_image_embeds.to(self.device, dtype=torch.float16)\n",
    "        image_prompt_embeds = self.image_proj_model(clip_image_embeds)\n",
    "        uncond_image_prompt_embeds = self.image_proj_model(torch.zeros_like(clip_image_embeds))\n",
    "        return image_prompt_embeds, uncond_image_prompt_embeds\n",
    "\n",
    "    def set_scale(self, scale):\n",
    "        for attn_processor in self.pipe.unet.attn_processors.values():\n",
    "            if isinstance(attn_processor, IPAttnProcessor):\n",
    "                attn_processor.scale = scale\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        pil_image=None,\n",
    "        clip_image_embeds=None,\n",
    "        prompt=None,\n",
    "        negative_prompt=None,\n",
    "        scale=1.0,\n",
    "        num_samples=4,\n",
    "        seed=None,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=30,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.set_scale(scale)\n",
    "\n",
    "        if pil_image is not None:\n",
    "            num_prompts = 1 if isinstance(pil_image, Image.Image) else len(pil_image)\n",
    "        else:\n",
    "            num_prompts = clip_image_embeds.size(0)\n",
    "\n",
    "        if prompt is None:\n",
    "            prompt = \"best quality, high quality\"\n",
    "        if negative_prompt is None:\n",
    "            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "\n",
    "        if not isinstance(prompt, List):\n",
    "            prompt = [prompt] * num_prompts\n",
    "        if not isinstance(negative_prompt, List):\n",
    "            negative_prompt = [negative_prompt] * num_prompts\n",
    "\n",
    "        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(\n",
    "            pil_image=pil_image, clip_image_embeds=clip_image_embeds\n",
    "        )\n",
    "        bs_embed, seq_len, _ = image_prompt_embeds.shape\n",
    "        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            prompt_embeds_, negative_prompt_embeds_ = self.pipe.encode_prompt(\n",
    "                prompt,\n",
    "                device=self.device,\n",
    "                num_images_per_prompt=num_samples,\n",
    "                do_classifier_free_guidance=True,\n",
    "                negative_prompt=negative_prompt,\n",
    "            )\n",
    "            prompt_embeds = torch.cat([prompt_embeds_, image_prompt_embeds], dim=1)\n",
    "            negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_image_prompt_embeds], dim=1)\n",
    "\n",
    "        generator = get_generator(seed, self.device)\n",
    "\n",
    "        images = self.pipe(\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            generator=generator,\n",
    "            **kwargs,\n",
    "        ).images\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b20d815",
   "metadata": {},
   "source": [
    "## Image projection model\n",
    "代码中设计了三种不同的图像投影模型\n",
    "- ImageProjModel：基础 IP-Adapter 中使用的简单线性投影模型\n",
    "- MLPProjModel：在 IP-AdapterFull 中使用多层感知器进行更复杂的投影\n",
    "- Resampler：在 IP-AdapterPlus 变体中用于细粒度特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0902b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class ImageProjModel(torch.nn.Module):\n",
    "    \"\"\"Projection Model\"\"\"\n",
    "\n",
    "    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = None\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.clip_extra_context_tokens = clip_extra_context_tokens\n",
    "        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n",
    "        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n",
    "\n",
    "    def forward(self, image_embeds):\n",
    "        embeds = image_embeds\n",
    "        # 使用简单的单线性映射层+LayerNorm\n",
    "        # -----------------------------------------------\n",
    "        clip_extra_context_tokens = self.proj(embeds).reshape(\n",
    "            -1, self.clip_extra_context_tokens, self.cross_attention_dim\n",
    "        )\n",
    "        clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n",
    "        # -----------------------------------------------\n",
    "        return clip_extra_context_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad71b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPProjModel(torch.nn.Module):\n",
    "    \"\"\"SD model with image prompt\"\"\"\n",
    "    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024):\n",
    "        super().__init__()\n",
    "        # 使用相对复杂的Linear+GELU+Linear+LayeNorm进行映射\n",
    "        # ------------------------------------------------\n",
    "        self.proj = torch.nn.Sequential(\n",
    "            torch.nn.Linear(clip_embeddings_dim, clip_embeddings_dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(clip_embeddings_dim, cross_attention_dim),\n",
    "            torch.nn.LayerNorm(cross_attention_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, image_embeds):\n",
    "        clip_extra_context_tokens = self.proj(image_embeds)\n",
    "        return clip_extra_context_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee97208",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class Resampler(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=1024,\n",
    "        depth=8,\n",
    "        dim_head=64,\n",
    "        heads=16,\n",
    "        num_queries=8,\n",
    "        embedding_dim=768,\n",
    "        output_dim=1024,\n",
    "        ff_mult=4,\n",
    "        max_seq_len: int = 257,  # CLIP tokens + CLS token\n",
    "        apply_pos_emb: bool = False,\n",
    "        num_latents_mean_pooled: int = 0,  # number of latents derived from mean pooled representation of the sequence\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, embedding_dim) if apply_pos_emb else None\n",
    "\n",
    "        self.latents = nn.Parameter(torch.randn(1, num_queries, dim) / dim**0.5)\n",
    "\n",
    "        self.proj_in = nn.Linear(embedding_dim, dim)\n",
    "\n",
    "        self.proj_out = nn.Linear(dim, output_dim)\n",
    "        self.norm_out = nn.LayerNorm(output_dim)\n",
    "\n",
    "        self.to_latents_from_mean_pooled_seq = (\n",
    "            nn.Sequential(\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.Linear(dim, dim * num_latents_mean_pooled),\n",
    "                Rearrange(\"b (n d) -> b n d\", n=num_latents_mean_pooled),\n",
    "            )\n",
    "            if num_latents_mean_pooled > 0\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n",
    "                        FeedForward(dim=dim, mult=ff_mult),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pos_emb is not None:\n",
    "            n, device = x.shape[1], x.device\n",
    "            pos_emb = self.pos_emb(torch.arange(n, device=device))\n",
    "            x = x + pos_emb\n",
    "\n",
    "        latents = self.latents.repeat(x.size(0), 1, 1)\n",
    "\n",
    "        x = self.proj_in(x)\n",
    "\n",
    "        if self.to_latents_from_mean_pooled_seq:\n",
    "            meanpooled_seq = masked_mean(x, dim=1, mask=torch.ones(x.shape[:2], device=x.device, dtype=torch.bool))\n",
    "            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n",
    "            latents = torch.cat((meanpooled_latents, latents), dim=-2)\n",
    "\n",
    "        for attn, ff in self.layers:\n",
    "            latents = attn(x, latents) + latents\n",
    "            latents = ff(latents) + latents\n",
    "\n",
    "        latents = self.proj_out(latents)\n",
    "        return self.norm_out(latents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0e774f",
   "metadata": {},
   "source": [
    "## 注意力机制修改\n",
    "- IPAttnProcessor: 修改交叉注意力以包含图像嵌入\n",
    "- CNAttnProcessor: 控制网集成的特殊处理器(for control-net layers)\n",
    "- AttnProcessor: 标准处理器，用于没有交叉注意力的层(for non-cross-attention layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62589580",
   "metadata": {},
   "source": [
    "IP-adapter类中设置注意力机制的函数——set_ip_adapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51643fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ip_adapter(self):\n",
    "        unet = self.pipe.unet\n",
    "        attn_procs = {}\n",
    "        for name in unet.attn_processors.keys():\n",
    "            # 如果以attn.processor结尾，则说明是self-attn模块，否则说明是cross_attn\n",
    "            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "            # 如果是mid_block,则取最后一层\n",
    "            if name.startswith(\"mid_block\"):\n",
    "                hidden_size = unet.config.block_out_channels[-1]\n",
    "            # 如果是\n",
    "            elif name.startswith(\"up_blocks\"):\n",
    "                block_id = int(name[len(\"up_blocks.\")])\n",
    "                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "            # 如果是down_blocks，则\n",
    "            elif name.startswith(\"down_blocks\"):\n",
    "                block_id = int(name[len(\"down_blocks.\")])\n",
    "                hidden_size = unet.config.block_out_channels[block_id]\n",
    "            if cross_attention_dim is None:\n",
    "                attn_procs[name] = AttnProcessor()\n",
    "            else:\n",
    "                attn_procs[name] = IPAttnProcessor(\n",
    "                    hidden_size=hidden_size,\n",
    "                    cross_attention_dim=cross_attention_dim,\n",
    "                    scale=1.0,\n",
    "                    num_tokens=self.num_tokens,\n",
    "                ).to(self.device, dtype=torch.float16)\n",
    "        unet.set_attn_processor(attn_procs)\n",
    "        if hasattr(self.pipe, \"controlnet\"):\n",
    "            if isinstance(self.pipe.controlnet, MultiControlNetModel):\n",
    "                for controlnet in self.pipe.controlnet.nets:\n",
    "                    controlnet.set_attn_processor(CNAttnProcessor(num_tokens=self.num_tokens))\n",
    "            else:\n",
    "                self.pipe.controlnet.set_attn_processor(CNAttnProcessor(num_tokens=self.num_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9853a37",
   "metadata": {},
   "source": [
    "### IPAttnProcessor\n",
    "- encoder hidden state:文本隐藏状态\n",
    "- ip hidden state：图像隐藏状态\n",
    "- 加权组合：`hidden_states = hidden_states + self.scale * ip_hidden_states`\n",
    "- linear proj + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b23e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPAttnProcessor(nn.Module):\n",
    "    r\"\"\"\n",
    "    Attention processor for IP-Adapater.\n",
    "    Args:\n",
    "        hidden_size (`int`):\n",
    "            The hidden size of the attention layer.\n",
    "        cross_attention_dim (`int`):\n",
    "            The number of channels in the `encoder_hidden_states`.\n",
    "        scale (`float`, defaults to 1.0):\n",
    "            the weight scale of image prompt.\n",
    "        num_tokens (`int`, defaults to 4 when do ip_adapter_plus it should be 16):\n",
    "            The context length of the image features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, cross_attention_dim=None, scale=1.0, num_tokens=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.scale = scale\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n",
    "        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "        )\n",
    "        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        else:\n",
    "            # get encoder_hidden_states, ip_hidden_states\n",
    "            end_pos = encoder_hidden_states.shape[1] - self.num_tokens\n",
    "            encoder_hidden_states, ip_hidden_states = (\n",
    "                encoder_hidden_states[:, :end_pos, :],\n",
    "                encoder_hidden_states[:, end_pos:, :],\n",
    "            )\n",
    "            if attn.norm_cross:\n",
    "                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "        # 计算普通注意力\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        query = attn.head_to_batch_dim(query)\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "\n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "\n",
    "        # 计算图像提示注意力\n",
    "        ip_key = self.to_k_ip(ip_hidden_states)\n",
    "        ip_value = self.to_v_ip(ip_hidden_states)\n",
    "\n",
    "        ip_key = attn.head_to_batch_dim(ip_key)\n",
    "        ip_value = attn.head_to_batch_dim(ip_value)\n",
    "\n",
    "        ip_attention_probs = attn.get_attention_scores(query, ip_key, None)\n",
    "        self.attn_map = ip_attention_probs\n",
    "        ip_hidden_states = torch.bmm(ip_attention_probs, ip_value)\n",
    "        ip_hidden_states = attn.batch_to_head_dim(ip_hidden_states)\n",
    "\n",
    "        # 加权组合\n",
    "        # --------------------------------------------------------------\n",
    "        hidden_states = hidden_states + self.scale * ip_hidden_states\n",
    "        # --------------------------------------------------------------\n",
    "\n",
    "        \n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9d27e",
   "metadata": {},
   "source": [
    "### CNAttnProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a23eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNAttnProcessor:\n",
    "    r\"\"\"\n",
    "    Default processor for performing attention-related computations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_tokens=4):\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None, temb=None, *args, **kwargs,):\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "        )\n",
    "        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        else:\n",
    "            end_pos = encoder_hidden_states.shape[1] - self.num_tokens\n",
    "            encoder_hidden_states = encoder_hidden_states[:, :end_pos]  # only use text\n",
    "            if attn.norm_cross:\n",
    "                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        query = attn.head_to_batch_dim(query)\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "\n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769a356",
   "metadata": {},
   "source": [
    "## IPAdapterFaceID\n",
    "- IPAdapterFaceID\n",
    "- IPAdapterFaceIDPlus\n",
    "- IPAdapterFaceIDXL\n",
    "- IPAdapterFaceIDPlusXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "746cf8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class IPAdapterFaceID:\n",
    "    def __init__(self, sd_pipe, ip_ckpt, device, lora_rank=128, num_tokens=4, torch_dtype=torch.float16):\n",
    "        self.device = device\n",
    "        self.ip_ckpt = ip_ckpt\n",
    "        self.lora_rank = lora_rank\n",
    "        self.num_tokens = num_tokens\n",
    "        self.torch_dtype = torch_dtype\n",
    "\n",
    "        self.pipe = sd_pipe.to(self.device)\n",
    "        self.set_ip_adapter()\n",
    "\n",
    "        # image proj model\n",
    "        self.image_proj_model = self.init_proj()\n",
    "\n",
    "        self.load_ip_adapter()\n",
    "\n",
    "    def init_proj(self):\n",
    "        image_proj_model = MLPProjModel(\n",
    "            cross_attention_dim=self.pipe.unet.config.cross_attention_dim,\n",
    "            id_embeddings_dim=512,\n",
    "            num_tokens=self.num_tokens,\n",
    "        ).to(self.device, dtype=self.torch_dtype)\n",
    "        return image_proj_model\n",
    "\n",
    "    def set_ip_adapter(self):\n",
    "        unet = self.pipe.unet\n",
    "        attn_procs = {}\n",
    "        for name in unet.attn_processors.keys():\n",
    "            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "            if name.startswith(\"mid_block\"):\n",
    "                hidden_size = unet.config.block_out_channels[-1]\n",
    "            elif name.startswith(\"up_blocks\"):\n",
    "                block_id = int(name[len(\"up_blocks.\")])\n",
    "                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "            elif name.startswith(\"down_blocks\"):\n",
    "                block_id = int(name[len(\"down_blocks.\")])\n",
    "                hidden_size = unet.config.block_out_channels[block_id]\n",
    "            if cross_attention_dim is None:\n",
    "                attn_procs[name] = LoRAAttnProcessor(\n",
    "                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, rank=self.lora_rank,\n",
    "                ).to(self.device, dtype=self.torch_dtype)\n",
    "            else:\n",
    "                attn_procs[name] = LoRAIPAttnProcessor(\n",
    "                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, scale=1.0, rank=self.lora_rank, num_tokens=self.num_tokens,\n",
    "                ).to(self.device, dtype=self.torch_dtype)\n",
    "        unet.set_attn_processor(attn_procs)\n",
    "\n",
    "    def load_ip_adapter(self):\n",
    "        if os.path.splitext(self.ip_ckpt)[-1] == \".safetensors\":\n",
    "            state_dict = {\"image_proj\": {}, \"ip_adapter\": {}}\n",
    "            with safe_open(self.ip_ckpt, framework=\"pt\", device=\"cpu\") as f:\n",
    "                for key in f.keys():\n",
    "                    if key.startswith(\"image_proj.\"):\n",
    "                        state_dict[\"image_proj\"][key.replace(\"image_proj.\", \"\")] = f.get_tensor(key)\n",
    "                    elif key.startswith(\"ip_adapter.\"):\n",
    "                        state_dict[\"ip_adapter\"][key.replace(\"ip_adapter.\", \"\")] = f.get_tensor(key)\n",
    "        else:\n",
    "            state_dict = torch.load(self.ip_ckpt, map_location=\"cpu\")\n",
    "        self.image_proj_model.load_state_dict(state_dict[\"image_proj\"])\n",
    "        ip_layers = torch.nn.ModuleList(self.pipe.unet.attn_processors.values())\n",
    "        ip_layers.load_state_dict(state_dict[\"ip_adapter\"])\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def get_image_embeds(self, faceid_embeds):\n",
    "        \n",
    "        faceid_embeds = faceid_embeds.to(self.device, dtype=self.torch_dtype)\n",
    "        image_prompt_embeds = self.image_proj_model(faceid_embeds)\n",
    "        uncond_image_prompt_embeds = self.image_proj_model(torch.zeros_like(faceid_embeds))\n",
    "        return image_prompt_embeds, uncond_image_prompt_embeds\n",
    "\n",
    "    def set_scale(self, scale):\n",
    "        for attn_processor in self.pipe.unet.attn_processors.values():\n",
    "            # 使用lora合并\n",
    "            if isinstance(attn_processor, LoRAIPAttnProcessor):\n",
    "                attn_processor.scale = scale\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        faceid_embeds=None,\n",
    "        prompt=None,\n",
    "        negative_prompt=None,\n",
    "        scale=1.0,\n",
    "        num_samples=4,\n",
    "        seed=None,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=30,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.set_scale(scale)\n",
    "\n",
    "       \n",
    "        num_prompts = faceid_embeds.size(0)\n",
    "            negative_prompt = [negative_prompt] * num_prompts\n",
    "\n",
    "        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(faceid_embeds)\n",
    "\n",
    "        bs_embed, seq_len, _ = image_prompt_embeds.shape\n",
    "        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        if not isinstance(negative_prompt, List):\n",
    "            negative_prompt = [negative_prompt] * num_prompts\n",
    "        # 获得cond和uncond的embed\n",
    "        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(faceid_embeds)\n",
    "        # batch_size,seq_len,_\n",
    "        bs_embed, seq_len, _ = image_prompt_embeds.shape\n",
    "        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            prompt_embeds_, negative_prompt_embeds_ = self.pipe.encode_prompt(\n",
    "                prompt,\n",
    "                device=self.device,\n",
    "                num_images_per_prompt=num_samples,\n",
    "                do_classifier_free_guidance=True,\n",
    "                negative_prompt=negative_prompt,\n",
    "            )\n",
    "            prompt_embeds = torch.cat([prompt_embeds_, image_prompt_embeds], dim=1)\n",
    "            negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_image_prompt_embeds], dim=1)\n",
    "\n",
    "        generator = get_generator(seed, self.device)\n",
    "\n",
    "        images = self.pipe(\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            generator=generator,\n",
    "            **kwargs,\n",
    "        ).images\n",
    "\n",
    "        return images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78759b4e",
   "metadata": {},
   "source": [
    "### LoRAAttnProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca12aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from diffusers.models.lora import LoRALinearLayer\n",
    "\n",
    "\n",
    "class LoRAAttnProcessor(nn.Module):\n",
    "    r\"\"\"\n",
    "    Default processor for performing attention-related computations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=None,\n",
    "        cross_attention_dim=None,\n",
    "        rank=4,\n",
    "        network_alpha=None,\n",
    "        lora_scale=1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rank = rank\n",
    "        self.lora_scale = lora_scale\n",
    "        \n",
    "        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n",
    "        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n",
    "        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n",
    "        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "        )\n",
    "        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states) + self.lora_scale * self.to_q_lora(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states) + self.lora_scale * self.to_k_lora(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states) + self.lora_scale * self.to_v_lora(encoder_hidden_states)\n",
    "\n",
    "        query = attn.head_to_batch_dim(query)\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "\n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states) + self.lora_scale * self.to_out_lora(hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class LoRAIPAttnProcessor(nn.Module):\n",
    "    r\"\"\"\n",
    "    Attention processor for IP-Adapater.\n",
    "    Args:\n",
    "        hidden_size (`int`):\n",
    "            The hidden size of the attention layer.\n",
    "        cross_attention_dim (`int`):\n",
    "            The number of channels in the `encoder_hidden_states`.\n",
    "        scale (`float`, defaults to 1.0):\n",
    "            the weight scale of image prompt.\n",
    "        num_tokens (`int`, defaults to 4 when do ip_adapter_plus it should be 16):\n",
    "            The context length of the image features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, cross_attention_dim=None, rank=4, network_alpha=None, lora_scale=1.0, scale=1.0, num_tokens=4):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.scale = scale\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n",
    "        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n",
    "\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.scale = scale\n",
    "        self.num_tokens = num_tokens\n",
    "        # -----------------------------------------------------------------------------\n",
    "        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n",
    "        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n",
    "        # -----------------------------------------------------------------------------\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "        )\n",
    "        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states) + self.lora_scale * self.to_q_lora(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        else:\n",
    "            # get encoder_hidden_states, ip_hidden_states\n",
    "            end_pos = encoder_hidden_states.shape[1] - self.num_tokens\n",
    "            encoder_hidden_states, ip_hidden_states = (\n",
    "                encoder_hidden_states[:, :end_pos, :],\n",
    "                encoder_hidden_states[:, end_pos:, :],\n",
    "            )\n",
    "            if attn.norm_cross:\n",
    "                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "        # LoRA注意力：原始权重 + 低秩适应矩阵\n",
    "        # --------------------------------------------------------------------------------------------------\n",
    "        key = attn.to_k(encoder_hidden_states) + self.lora_scale * self.to_k_lora(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states) + self.lora_scale * self.to_v_lora(encoder_hidden_states)\n",
    "        # -------------------------------------------------------------------------------------------------\n",
    "\n",
    "        query = attn.head_to_batch_dim(query)\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "\n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "\n",
    "        # for ip-adapter\n",
    "        ip_key = self.to_k_ip(ip_hidden_states)\n",
    "        ip_value = self.to_v_ip(ip_hidden_states)\n",
    "\n",
    "        ip_key = attn.head_to_batch_dim(ip_key)\n",
    "        ip_value = attn.head_to_batch_dim(ip_value)\n",
    "\n",
    "        ip_attention_probs = attn.get_attention_scores(query, ip_key, None)\n",
    "        self.attn_map = ip_attention_probs\n",
    "        ip_hidden_states = torch.bmm(ip_attention_probs, ip_value)\n",
    "        ip_hidden_states = attn.batch_to_head_dim(ip_hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states + self.scale * ip_hidden_states\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states) + self.lora_scale * self.to_out_lora(hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9964fc6",
   "metadata": {},
   "source": [
    "想要自己训练一下，但是训练资源不太允许~~\n",
    "- Standard IP-Adapter: 16GB+ VRAM GPU\n",
    "- IP-Adapter Plus: 24GB+ VRAM GPU\n",
    "- IP-Adapter SDXL: 32GB+ VRAM GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3e6571",
   "metadata": {},
   "source": [
    "\n",
    "## 训练与推理\n",
    "### 训练\n",
    "我们仅优化 IP-Adapter，同时保持预训练扩散模型的参数不变。IP-Adapter 在带有一对多图像-文本对的数据集上进行训练，使用与原始 SD 相同的目标函数：\n",
    "\n",
    "$$\n",
    "L_{\\text{simple}} = \\mathbb{E}_{\\boldsymbol{x}_0, \\epsilon, c_t, c_i} \\|\\epsilon - \\epsilon_{\\theta}\\bigl(\\boldsymbol{x}_t, c_t, c_i, t\\bigr)\\|^2.\n",
    "$$\n",
    "\n",
    "为了在推理阶段实现无条件引导，我们在训练阶段随机丢弃图像条件：\n",
    "\n",
    "$$\n",
    "\\hat{\\epsilon}_{\\theta}\\bigl(\\boldsymbol{x}_t, c_t, c_i, t\\bigr) = w \\epsilon_{\\theta}\\bigl(\\boldsymbol{x}_t, c_t, c_i, t\\bigr) + (1 - w) \\epsilon_{\\theta}\\bigl(\\boldsymbol{x}_t, t\\bigr)\n",
    "$$\n",
    "\n",
    "丢弃时，我们直接将 CLIP 图像嵌入置零。\n",
    "\n",
    "### 推理\n",
    "由于文本交叉注意力和图像交叉注意力是分开计算的，我们在推理阶段可以调整图像条件的权重：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{Z}^{\\text{new}} = \\text{Attention}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}) + \\lambda \\cdot \\text{Attention}(\\boldsymbol{Q}, \\boldsymbol{K'}, \\boldsymbol{V'})\n",
    "$$\n",
    "\n",
    "其中，$\\lambda$ 是权重因子。当 $\\lambda = 0$ 时，模型还原为原始的文本到图像扩散模型(only text-guide)。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b366e1b8",
   "metadata": {},
   "source": [
    "# Diffusers usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445dc8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, IPAdapter\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "base_model_id = \"g:code/model/stable-diffusion-v1-5\"\n",
    "ip_adapter_model_id = \"h94/IP-Adapter\" # 标准IP-Adapter Standard IP-Adapter\n",
    "\n",
    "# 加载基础Stable Diffusion管线\n",
    "# Load the base Stable Diffusion pipeline\n",
    "# 使用float16以节省显存 Use float16 to save VRAM\n",
    "pipe = StableDiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16)\n",
    "# 将管线移动到GPU（如果可用）Move pipeline to GPU (if available)\n",
    "pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载IP-Adapter权重\n",
    "# Load IP-Adapter weights\n",
    "# 标准IP-Adapter的权重文件通常在'models'子文件夹下\n",
    "# Standard IP-Adapter weights are usually in the 'models' subfolder\n",
    "ip_adapter = IPAdapter.from_pretrained(ip_adapter_model_id, subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\", torch_dtype=torch.float16)\n",
    "\n",
    "# 将IP-Adapter添加到管线中\n",
    "# Add the IP-Adapter to the pipeline\n",
    "pipe.load_ip_adapter(ip_adapter)\n",
    "\n",
    "# 加载用于风格/内容参考的图像\n",
    "# Load the image for style/content reference\n",
    "# 请替换为你的图像文件路径 Please replace with your image file path\n",
    "image_path = \"path/to/your/input_image.jpg\" # <-- 请修改这里 Please modify here\n",
    "try:\n",
    "    input_image = Image.open(image_path)\n",
    "    # 确保图像是RGB格式 Ensure image is in RGB format\n",
    "    if input_image.mode != \"RGB\":\n",
    "        input_image = input_image.convert(\"RGB\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"错误：未找到输入图像文件 {image_path}\")\n",
    "    print(\"Error: Input image file not found at {image_path}\")\n",
    "    # 如果文件未找到，可以考虑退出或跳过 If file not found, consider exiting or skipping\n",
    "    input_image = None # Set to None to indicate failure\n",
    "\n",
    "if input_image is not None:\n",
    "    # 定义文本提示词\n",
    "    # Define the text prompt\n",
    "    prompt = \"a person, wearing a hat\" # <-- 请修改这里 Please modify here\n",
    "\n",
    "    # 生成图像\n",
    "    # Generate the image\n",
    "    print(\"正在生成图像...\")\n",
    "    print(\"Generating image...\")\n",
    "    # 调用管线，传入提示词和参考图像\n",
    "    # Call the pipeline, passing the prompt and the reference image\n",
    "    # 可以调整 scale 参数来控制IP-Adapter的影响强度\n",
    "    # You can adjust the scale parameter to control the strength of the IP-Adapter influence\n",
    "    generated_image = pipe(prompt=prompt, ip_adapter_image=input_image, negative_prompt=\"bad anatomy, blurry, low resolution, worst quality\", scale=1.0).images[0]\n",
    "\n",
    "    # 显示或保存生成的图像\n",
    "    # Display or save the generated image\n",
    "    print(\"图像生成完成。\")\n",
    "    print(\"Image generation complete.\")\n",
    "    # generated_image.save(\"generated_image.png\") # 取消注释以保存图像 Uncomment to save the image\n",
    "    display(generated_image) # 在Jupyter Notebook中显示图像 Display image in Jupyter Notebook\n",
    "else:\n",
    "    print(\"由于未找到输入图像，跳过图像生成。\")\n",
    "    print(\"Skipping image generation due to missing input image.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heavy_daily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
