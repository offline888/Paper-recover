{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关论文\n",
    "- 《Sana: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer》\n",
    "- 《Sana1.5:Efficient Scaling of Training-Time and\n",
    "Inference-Time Compute in Linear Diffusion Transformer》"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github仓库README信息简要\n",
    "- 标准模型：0.6B 模型需 9GB VRAM，1.6B 模型需 12GB VRAM.训练需要32GB VRAM。4 位量化模型最低需 8GB VRAM。\n",
    "- Model \n",
    "    - Sana-1.5 实现了高效的模型扩展策略，在提高质量的同时保持了合理的计算需求。它引入了深度增长范式和模型剪枝技术\n",
    "    - Sana-Sprint 是一种专注于时间步蒸馏的专门变体，仅需 1-4 次推理步骤即可实现高质量的生成，显著减少了生成时间。\n",
    "- Inference & Test Metrics (FID, CLIP Score, GenEval, DPG-Bench, etc...)\n",
    "- **Inference Scaling**:Sana 可以使用专门的 NVILA-2B 模型（称为 VISA）对候选图像进行评分，从而从多个生成中选择最高质量的结果，显著提高性能指标.(这算哪门子Inference Scaling.....)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | 分辨率 | 参数（B） | 延迟（秒） | 吞吐量（张/秒） | FID ↓ | CLIP 得分 ↑ | GenEval ↑ |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Sana-0.6B | 1024×1024 | 0.6 | 0.9 | 1.7 | 5.61 | 28.80 | 0.68 |\n",
    "| Sana-1.6B | 1024×1024 | 1.6 | 1.2 | 1.0 | 5.76 | 28.67 | 0.66 |\n",
    "| Sana-1.5-1.6B | 1024×1024 | 1.6 | 1.2 | 1.0 | 5.70 | 29.12 | 0.82 |\n",
    "| Sana-1.5-4.8B | 1024×1024 | 4.8 | 4.2 | 0.26 | 5.99 | 29.23 | 0.81 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SANA-0.6B\n",
    "    - 512px\n",
    "    - 1024px\n",
    "    - ControlNet\n",
    "- SANA-1.6B\n",
    "    - 512px\n",
    "    - 1024px\n",
    "    - 2Kpx\n",
    "    - 4Kpx\n",
    "    - ControlNet\n",
    "- SANA1.5-1.6B\n",
    "- SANA1.5-4.8B\n",
    "- SANA-Sprint\n",
    "    - Sana-Sprint-0.6B\n",
    "    - Sana-Sprint-1.6B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model Variant | Depth | Hidden Size | Patch Size | Num Heads | Parameters |\n",
    "|---|---|---|---|---|---|\n",
    "| Sana-0.6B | 28 | 1152 | 1 or 2 | 16 | 600M |\n",
    "| Sana-1.6B | 20 | 2240 | 1 or 2 | 20 | 1.6B |\n",
    "| Sana-4.8B | 60 | 2240 | 1 | 20 | 4.8B |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型架构剖析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../Image/sana.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear DiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**标准自注意力（Self-Attention）的时间复杂度**:  $O(n^2 d)$ <p>\n",
    "**线性注意力的时间复杂度**：$O(nd^2)$，如果d固定不变就是关于n线性的<p>\n",
    "其中，$n$ 表示序列长度（例如，图像块的数量），$d$ 表示隐藏维度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import SanaTransformer2DModel\n",
    "import torch\n",
    "# 根据错误提示，尝试添加 low_cpu_mem_usage=False 和 device_map=None 参数\n",
    "model=SanaTransformer2DModel.from_pretrained(r\"G:\\code\\model\\SANA1.5_1.6B_1024px_diffusers\\transformer\",\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            low_cpu_mem_usage=True,\n",
    "                                            device_map=None).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载的 DiT 模型总参数量: 1,604,462,752 个\n"
     ]
    }
   ],
   "source": [
    "# 统计模型参数\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"加载的 DiT 模型总参数量: {total_params:,} 个\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出1.6B的Sana是20个相同的Transformer块堆叠而成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embed\n",
      "  - proj\n",
      "time_embed\n",
      "  - emb\n",
      "  - silu\n",
      "  - linear\n",
      "caption_projection\n",
      "  - linear_1\n",
      "  - act_1\n",
      "  - linear_2\n",
      "caption_norm\n",
      "transformer_blocks\n",
      "  - 0\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "  - 6\n",
      "  - 7\n",
      "  - 8\n",
      "  - 9\n",
      "  - 10\n",
      "  - 11\n",
      "  - 12\n",
      "  - 13\n",
      "  - 14\n",
      "  - 15\n",
      "  - 16\n",
      "  - 17\n",
      "  - 18\n",
      "  - 19\n",
      "norm_out\n",
      "proj_out\n"
     ]
    }
   ],
   "source": [
    "for name, child in model.named_children():\n",
    "    print(name) # 打印当前模块的名称\n",
    "    # 遍历当前模块的子模块\n",
    "    for sub_name, sub_child in child.named_children():\n",
    "        print(f\"  - {sub_name}\") # 打印子模块的名称，使用缩进表示层级关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n",
      "<class 'diffusers.models.transformers.sana_transformer.SanaTransformerBlock'>\n"
     ]
    }
   ],
   "source": [
    "for name,child in model.transformer_blocks.named_children():\n",
    "    print(type(child))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要的参数也是来自于这个Transformer块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiT中Transformer块模型总参数量: 1,558,412,800 个\n"
     ]
    }
   ],
   "source": [
    "# 统计模型参数\n",
    "total_params = sum(p.numel() for p in model.transformer_blocks.parameters())\n",
    "print(f\"DiT中Transformer块模型总参数量: {total_params:,} 个\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SanaTransformerBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Transformer block introduced in [Sana](https://huggingface.co/papers/2410.10629).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int = 2240,\n",
    "        num_attention_heads: int = 70,\n",
    "        attention_head_dim: int = 32,\n",
    "        dropout: float = 0.0,\n",
    "        num_cross_attention_heads: Optional[int] = 20,\n",
    "        cross_attention_head_dim: Optional[int] = 112,\n",
    "        cross_attention_dim: Optional[int] = 2240,\n",
    "        attention_bias: bool = True,\n",
    "        norm_elementwise_affine: bool = False,\n",
    "        norm_eps: float = 1e-6,\n",
    "        attention_out_bias: bool = True,\n",
    "        mlp_ratio: float = 2.5,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Self Attention\n",
    "        self.norm1 = nn.LayerNorm(dim, elementwise_affine=False, eps=norm_eps)\n",
    "        self.attn1 = Attention(\n",
    "            query_dim=dim,\n",
    "            heads=num_attention_heads,\n",
    "            dim_head=attention_head_dim,\n",
    "            dropout=dropout,\n",
    "            bias=attention_bias,\n",
    "            cross_attention_dim=None,\n",
    "            processor=SanaLinearAttnProcessor2_0(),\n",
    "        )\n",
    "\n",
    "        # 2. Cross Attention\n",
    "        if cross_attention_dim is not None:\n",
    "            self.norm2 = nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine, eps=norm_eps)\n",
    "            self.attn2 = Attention(\n",
    "                query_dim=dim,\n",
    "                cross_attention_dim=cross_attention_dim,\n",
    "                heads=num_cross_attention_heads,\n",
    "                dim_head=cross_attention_head_dim,\n",
    "                dropout=dropout,\n",
    "                bias=True,\n",
    "                out_bias=attention_out_bias,\n",
    "                processor=AttnProcessor2_0(),\n",
    "            )\n",
    "\n",
    "        # 3. Feed-forward\n",
    "        self.ff = GLUMBConv(dim, dim, mlp_ratio, norm_type=None, residual_connection=False)\n",
    "\n",
    "        self.scale_shift_table = nn.Parameter(torch.randn(6, dim) / dim**0.5)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        timestep: Optional[torch.LongTensor] = None,\n",
    "        height: int = None,\n",
    "        width: int = None,\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size = hidden_states.shape[0]\n",
    "\n",
    "        # 1. Modulation：基于时间步生成调制参数\n",
    "        # 使用时间步生成六个调制参数：两个偏移(shift)、两个缩放(scale)和两个门控(gate)\n",
    "        # 使用gate_msa和gate_mlp控制自注意力和前馈网络输出的影响程度\n",
    "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = (\n",
    "            self.scale_shift_table[None] + timestep.reshape(batch_size, 6, -1)\n",
    "        ).chunk(6, dim=1)\n",
    "\n",
    "        # 2. Self Attention\n",
    "        norm_hidden_states = self.norm1(hidden_states)\n",
    "        # 使用参数调制\n",
    "        # ---------------------------------------------------------\n",
    "        norm_hidden_states = norm_hidden_states * (1 + scale_msa) + shift_msa\n",
    "        #-------------------------------------------------------------------\n",
    "        norm_hidden_states = norm_hidden_states.to(hidden_states.dtype)\n",
    "\n",
    "        attn_output = self.attn1(norm_hidden_states)\n",
    "        # 使用参数调制\n",
    "        # ---------------------------------------------------------\n",
    "        hidden_states = hidden_states + gate_msa * attn_output\n",
    "        #-------------------------------------------------------------------\n",
    "\n",
    "        # 3. Cross Attention\n",
    "        if self.attn2 is not None:\n",
    "            attn_output = self.attn2(\n",
    "                hidden_states,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                attention_mask=encoder_attention_mask,\n",
    "            )\n",
    "            hidden_states = attn_output + hidden_states\n",
    "\n",
    "        # 4. Feed-forward\n",
    "        norm_hidden_states = self.norm2(hidden_states)\n",
    "        # 使用参数调制\n",
    "        # ---------------------------------------------------------\n",
    "        norm_hidden_states = norm_hidden_states * (1 + scale_mlp) + shift_mlp\n",
    "        # ---------------------------------------------------------\n",
    "\n",
    "        norm_hidden_states = norm_hidden_states.unflatten(1, (height, width)).permute(0, 3, 1, 2)\n",
    "        ff_output = self.ff(norm_hidden_states)\n",
    "        ff_output = ff_output.flatten(2, 3).permute(0, 2, 1)\n",
    "        hidden_states = hidden_states + gate_mlp * ff_output\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 采用了类似PixArt的shift和scale参数调制特征，由时间步动态生成的\n",
    "- 门控控制机制：引入gate_msa和gate_mlp两个门控参数，用于控制自注意力和前馈网络对最终特征的贡献程度\n",
    "- 对Transformer块内不同组件(自注意力和前馈网络)分别进行调制，比简单的全局调制更精细和有效"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DC-AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "详见`dc-ae.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow-DPM-Slover"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heavy_daily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
