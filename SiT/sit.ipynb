{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers (SiT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant | Depth | Hidden Size | Patch Size | Heads\n",
    "--- | --- | --- | --- | ---\n",
    "SIT-XL | 28 | 1152 | 2/4/8 | 16\n",
    "SIT-L | 24 | 1024 | 2/4/8 | 16\n",
    "SIT-B | 12 | 768 | 2/4/8 | 12\n",
    "SiT-S | 12 | 384 | 2/4/8 | 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../Image/sit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 补丁嵌入：将输入图像或潜在表示转换为嵌入\n",
    "- 位置嵌入：使用正弦嵌入添加位置信息\n",
    "- 时间步长嵌入：将扩散时间步长嵌入到模型条件中\n",
    "- 标签嵌入器：嵌入类别标签，支持无分类器指导\n",
    "- SiT 块：带有自适应层归一化 (adaLN-Zero) 的 Transformer 块\n",
    "- 最终层：将特征转换回所需的输出空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIT的核心创新是连接了flow-based model和diffusion-based model，在数据和噪声分布之间进行转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整个pipeline对应的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def forward(self, x, t, y):\n",
    "        \"\"\"\n",
    "        SiT 的前向传播。\n",
    "        x: (N, C, H, W) 空间输入张量（图像或图像的潜在表示）\n",
    "        t: (N,) 扩散时间步长张量\n",
    "        y: (N,) 类别标签张量\n",
    "        \"\"\"\n",
    "        x = self.x_embedder(x) + self.pos_embed  # (N, T, D)，其中 T = H * W / patch_size ** 2，代表token number\n",
    "        t = self.t_embedder(t)                   # (N, D)\n",
    "        y = self.y_embedder(y, self.training)    # (N, D)\n",
    "        c = t + y                                # (N, D)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, c)                      # (N, T, D) 使用c进行modulate\n",
    "        x = self.final_layer(x, c)                # (N, T, patch_size ** 2 * out_channels)\n",
    "        x = self.unpatchify(x)                   # (N, out_channels, H, W)\n",
    "        if self.learn_sigma:\n",
    "            x, _ = x.chunk(2, dim=1)\n",
    "        return x\n",
    "\n",
    "    def forward_with_cfg(self, x, t, y, cfg_scale):\n",
    "        \"\"\"\n",
    "        SiT 的前向传播，同时也批量处理无条件前向传播以实现无分类器指导。\n",
    "        \"\"\"\n",
    "        # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb\n",
    "        half = x[: len(x) // 2]\n",
    "        combined = torch.cat([half, half], dim=0)\n",
    "        model_out = self.forward(combined, t, y)\n",
    "        # 为了精确重现默认情况下我们仅对三个通道应用无分类器指导。标准的 cfg 方法将其应用于所有通道。，\n",
    "        # 可以通过取消注释下一行并注释掉再下一行来实现。\n",
    "        # eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]\n",
    "        eps, rest = model_out[:, :3], model_out[:, 3:]\n",
    "        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
    "        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)\n",
    "        eps = torch.cat([half_eps, half_eps], dim=0)\n",
    "        return torch.cat([eps, rest], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    The final layer of SiT.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, patch_size, out_channels):\n",
    "        super().__init__()\n",
    "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True)\n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, 2 * hidden_size, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
    "        x = modulate(self.norm_final(x), shift, scale)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SiTBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A SiT block with adaptive layer norm zero (adaLN-Zero) conditioning.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, **block_kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True, **block_kwargs)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
    "        approx_gelu = lambda: nn.GELU(approximate=\"tanh\")\n",
    "        self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=0)\n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, 6 * hidden_size, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=1)\n",
    "        x = x + gate_msa.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
    "        x = x + gate_mlp.unsqueeze(1) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heavy_daily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
