{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Promptâ€”â€”ICLR 2025 Spotlight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è§£å†³T2Içš„ä¸»ä½“ä¸€è‡´æ€§é—®é¢˜ï¼Œå¯ä»¥è§†ä¸ºSuppressEOTçš„ç»­ä½œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»“åº“ä»£ç ç»„ä»¶\n",
    "- Consistent Image Generation Code: `main.py`\n",
    "- Gradio Code: `app.py`\n",
    "- Benchmark Generation Code: `resource/gen_benchmark.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Consitency in Text Embedding\n",
    "- Multiprompt generation:$[P_0;P_i],i\\in[1,N]$\n",
    "    - $P_0$:identity prompt,$P_i$:i-th frame prompt\n",
    "    - ç¬¬1å¸§: $[P_{0};P_{1}]$ â†’ â€œä¸€åªå¯çˆ±çš„æ°´å½©ç”»é£æ ¼çš„å°çŒ«åœ¨èŠ±å›­é‡Œâ€\n",
    "\n",
    "    - ç¬¬2å¸§: $[P_{0};P_{2}]$ â†’ â€œä¸€åªå¯çˆ±çš„æ°´å½©ç”»é£æ ¼çš„å°çŒ«ç©¿ç€è¶…äººæ–—ç¯·â€\n",
    "\n",
    "    - ç¬¬3å¸§: $[P_{0};P_{3}]$ â†’ â€œä¸€åªå¯çˆ±çš„æ°´å½©ç”»é£æ ¼çš„å°çŒ«æˆ´ç€é¡¹åœˆå’Œé“ƒé“›â€\n",
    "\n",
    "    - ç¬¬4å¸§: $[P_{0};P_{4}]$ â†’ â€œä¸€åªå¯çˆ±çš„æ°´å½©ç”»é£æ ¼çš„å°çŒ«ååœ¨ç¯®å­é‡Œâ€\n",
    "\n",
    "    - ç¬¬5å¸§: $[P_{0};P_{5}]$ â†’ â€œä¸€åªå¯çˆ±çš„æ°´å½©ç”»é£æ ¼çš„å°çŒ«ç©¿ç€å¯çˆ±çš„æ¯›è¡£â€\n",
    "- Singleprompt generation:$[P_0;P_1;P_2;...;P_N]$\n",
    "     - å•ä¸€æç¤º P: â€œä¸€åªå¯çˆ±çš„æ°´å½©ç”»é£æ ¼çš„å°çŒ«åœ¨èŠ±å›­é‡Œï¼Œç©¿ç€è¶…äººæ–—ç¯·ï¼Œæˆ´ç€é¡¹åœˆå’Œé“ƒé“›ï¼Œååœ¨ç¯®å­é‡Œï¼Œç©¿ç€å¯çˆ±çš„æ¯›è¡£â€,ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡è°ƒæ•´æ¯ä¸ªåœºæ™¯æè¿°çš„æƒé‡æ¥ç”Ÿæˆæ¯ä¸ªåœºæ™¯çš„å›¾åƒã€‚ä¾‹å¦‚ï¼š\n",
    "\n",
    "    - ç¬¬1å¸§ï¼šé€šè¿‡å¢å¼ºâ€œåœ¨èŠ±å›­é‡Œâ€çš„æƒé‡ï¼Œç”Ÿæˆâ€œä¸€åªå¯çˆ±çš„æ°´å½©ç”»é£æ ¼çš„å°çŒ«åœ¨èŠ±å›­é‡Œâ€çš„å›¾åƒã€‚\n",
    "\n",
    "    - ç¬¬2å¸§ï¼šé€šè¿‡å¢å¼ºâ€œç©¿ç€è¶…äººæ–—ç¯·â€çš„æƒé‡ï¼Œç”Ÿæˆâ€œä¸€åªå¯çˆ±çš„æ°´å½©ç”»é£æ ¼çš„å°çŒ«ç©¿ç€è¶…äººæ–—ç¯·â€çš„å›¾åƒã€‚\n",
    "\n",
    "    - ç¬¬3å¸§ï¼šé€šè¿‡å¢å¼ºâ€œæˆ´ç€é¡¹åœˆå’Œé“ƒé“›â€çš„æƒé‡ï¼Œç”Ÿæˆâ€œä¸€åªå¯çˆ±çš„æ°´å½©ç”»é£æ ¼çš„å°çŒ«æˆ´ç€é¡¹åœˆå’Œé“ƒé“›â€çš„å›¾åƒã€‚\n",
    "\n",
    "    - ç¬¬4å¸§ï¼šé€šè¿‡å¢å¼ºâ€œååœ¨ç¯®å­é‡Œâ€çš„æƒé‡ï¼Œç”Ÿæˆâ€œä¸€åªå¯çˆ±çš„æ°´å½©ç”»é£æ ¼çš„å°çŒ«ååœ¨ç¯®å­é‡Œâ€çš„å›¾åƒã€‚\n",
    "\n",
    "    - ç¬¬5å¸§ï¼šé€šè¿‡å¢å¼ºâ€œç©¿ç€å¯çˆ±çš„æ¯›è¡£â€çš„æƒé‡ï¼Œç”Ÿæˆâ€œä¸€åªå¯çˆ±çš„æ°´å½©ç”»é£æ ¼çš„å°çŒ«ç©¿ç€å¯çˆ±çš„æ¯›è¡£â€çš„å›¾åƒã€‚\n",
    "-  multiprompt embedding:$\\mathcal{C} _i= \\tau _\\xi ( [ \\mathcal{P} _0; \\mathcal{P} _i] ) =[c^{SOT},c^{\\mathcal{P}_0},c^{\\mathcal{P}_i},c^{EOT}],(i=1,\\ldots,N)$\n",
    "- singleprompt embedding:$\\mathcal{C}=\\tau_\\xi([\\mathcal{P}_0;\\mathcal{P}_1;\\ldots;\\mathcal{P}_N])=[\\boldsymbol{c}^{SOT},\\boldsymbol{c}^{\\mathcal{P}_0},\\boldsymbol{c}^{\\mathcal{P}_1},\\ldots,\\boldsymbol{c}^{\\mathcal{P}_N},\\boldsymbol{c}^{EOT}].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../../Image/t-SNE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Consitency in Imgae Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Prompt Reweightingï¼šä½¿ç”¨ä¸€ä¸ªç¼©æ”¾å› å­å¯¹ç‰¹å®šçš„embeedingè¿›è¡Œscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†**å¯è§†åŒ–å›¾åƒä¹‹é—´çš„èº«ä»½ç›¸ä¼¼æ€§**ï¼Œä½¿ç”¨Carvkitç§»é™¤èƒŒæ™¯ï¼Œä½¿ç”¨DINO-v2æå–è§†è§‰ç‰¹å¾ï¼Œç„¶åå°†è¿™äº›ç‰¹å¾é€šè¿‡t-SNEæŠ•å½±åˆ°2Dç©ºé—´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<../../Image/å±å¹•æˆªå›¾ 2025-05-26 131702.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "- Prompt Consolidation(above)\n",
    "- Singular Value Reweighting(SVR)\n",
    "- Identity-Preserving Cross-Attention (IPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Reweighting\n",
    "å»¶ç»­SupressEOTçš„æ€è·¯ï¼Œè¿›è¡Œæ”¹è¿›<p>\n",
    "å¥‡å¼‚å€¼é‡æ–°åŠ æƒï¼ˆSingular-Value Reweightingï¼‰çš„ä½¿ç”¨å¯ä»¥å‡å°‘å•æç¤ºç”Ÿæˆä¸­å¸§æè¿°çš„æ··åˆ\n",
    "- SVR+:$\\hat{\\sigma}=\\beta e^{\\alpha\\sigma}*\\sigma$\n",
    "- SVR-:$\\tilde{\\sigma}=\\beta^{\\prime}e^{-\\alpha^{\\prime}\\hat{\\sigma}}*\\hat{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swr_single_prompt_embeds(swr_words,prompt_embeds,prompt,tokenizer,alpha=1.0, beta=1.2, zero_eot=False):\n",
    "    # swr_words:Suppress words,æƒ³è¦å¼±åŒ–çš„è¯\n",
    "    # prompt:åŸå§‹çš„å®Œæ•´æ–‡æœ¬æç¤ºå­—ç¬¦ä¸²\n",
    "    # alpha,beta:punish_wightå‡½æ•°çš„å‚æ•°\n",
    "    # zero_eot:å†³å®šå¦‚ä½•å¤„ç†EOT tokençš„è¾“å…¥\n",
    "    punish_indices = []\n",
    "\n",
    "    prompt_tokens = prompt2tokens(tokenizer,prompt)\n",
    "    \n",
    "    words_tokens = prompt2tokens(tokenizer,swr_words)\n",
    "    words_tokens = [word for word in words_tokens if word != '<|endoftext|>' and word != '<|startoftext|>']\n",
    "    index_of_words = find_sublist_index(prompt_tokens,words_tokens)\n",
    "    \n",
    "    if index_of_words != -1:\n",
    "        punish_indices.extend([num for num in range(index_of_words, index_of_words+len(words_tokens))])\n",
    "    \n",
    "    if zero_eot:\n",
    "        # æ‰¾åˆ°token sequenceä¸­çš„æ‰€æœ‰EOTçš„ç´¢å¼•\n",
    "        eot_indices = [index for index, word in enumerate(prompt_tokens) if word == '<|endoftext|>']\n",
    "        # å°†EOT tokenç´¢å¼•å¯¹åº”çš„embeddingä¹˜0.9ï¼Œè¿›è¡Œå‰Šå¼±\n",
    "        prompt_embeds[eot_indices] *= 9e-1\n",
    "        pass\n",
    "    else:\n",
    "        punish_indices.extend([index for index, word in enumerate(prompt_tokens) if word == '<|endoftext|>'])\n",
    "\n",
    "    punish_indices = list(set(punish_indices))\n",
    "    # ä»å®Œæ•´çš„ prompt_embeds å¼ é‡ä¸­ï¼Œæå–å‡ºæ‰€æœ‰ç´¢å¼•åœ¨ punish_indices åˆ—è¡¨ä¸­çš„ token åµŒå…¥\n",
    "    # wo_batchï¼š[num_indices_to_punish, embedding_dim]\n",
    "    wo_batch = prompt_embeds[punish_indices]\n",
    "    # ------------------------------------------------------------------\n",
    "    wo_batch = punish_wight(wo_batch.T.to(float), \n",
    "                            wo_batch.size(0), \n",
    "                            alpha=alpha, \n",
    "                            beta=beta, \n",
    "                            calc_similarity=False).T.to(prompt_embeds.dtype)\n",
    "    # ------------------------------------------------------------------\n",
    "    prompt_embeds[punish_indices] = wo_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†promptè½¬æ¢ä¸ºinput_idsä¹‹åpaddingåˆ°max_lengthï¼Œç„¶ådecodeä¸ºtoken sequence\n",
    "def prompt2tokens(tokenizer, prompt):\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids\n",
    "    tokens = []\n",
    "    for text_input_id in text_input_ids[0]:\n",
    "        token = tokenizer.decoder[text_input_id.item()]\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cdist\n",
    "def punish_wight(tensor, latent_size, alpha=1.0, beta=1.2, calc_similarity=False):\n",
    "    u, s, vh = torch.linalg.svd(tensor)\n",
    "    u = u[:,:latent_size]\n",
    "    zero_idx = int(latent_size * alpha)\n",
    "\n",
    "    if calc_similarity:\n",
    "        _s = s.clone()\n",
    "        _s *= torch.exp(-alpha*_s) * beta\n",
    "        _s[zero_idx:] = 0\n",
    "        _tensor = u @ torch.diag(_s) @ vh\n",
    "        dist = cdist(tensor[:,0].unsqueeze(0).cpu(), _tensor[:,0].unsqueeze(0).cpu(), metric='cosine')\n",
    "        print(f'The distance between the word embedding before and after the punishment: {dist}')\n",
    "    # ------------------------------------------------------------\n",
    "    s *= torch.exp(-alpha*s) * beta\n",
    "    # ------------------------------------------------------------\n",
    "    tensor = u @ torch.diag(s) @ vh\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity-Preserving Cross-Attention \n",
    "- äº¤å‰æ³¨æ„åŠ›å›¾ï¼šèƒ½å¤Ÿæ•æ‰æ ‡è®°çš„ç‰¹å¾ä¿¡æ¯ï¼Œå³ä¸æ–‡æœ¬æç¤ºä¸­çš„ç‰¹å®šéƒ¨åˆ†ç›¸å…³çš„è¯­ä¹‰ä¿¡æ¯<p>\n",
    "  è‡ªæ³¨æ„åŠ›ï¼šä¿ç•™äº†å›¾åƒçš„å¸ƒå±€ä¿¡æ¯å’Œå½¢çŠ¶ç»†èŠ‚ï¼Œå³ä¸å›¾åƒçš„æ•´ä½“ç»“æ„å’Œå¤–è§‚ç›¸å…³çš„è§†è§‰ä¿¡æ¯<p>\n",
    "(From ã€ŠTowards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing (CVPR 2024)ã€‹)\n",
    "- https://github.com/alibaba/EasyNLP/tree/master/diffusion/FreePromptEditing\n",
    "- SVRåœ¨å•æç¤ºç”Ÿæˆä¸­å‡å°‘ä¸åŒå¸§æè¿°ä¹‹é—´çš„æ··åˆï¼Œä½†å®ƒå¯èƒ½å¯¹å•æç¤ºå†…çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§äº§ç”Ÿè´Ÿé¢å½±å“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVRä¹‹å,æˆ‘ä»¬å¾—åˆ°äº†æ›´æ–°åçš„$\\hat{C}$;åœ¨å»å™ªè¿‡ç¨‹ä¸­ï¼Œåœ¨Cross-attentionè¿‡ç¨‹å¾—åˆ°$\\hat{Q},\\hat{K},\\hat{V}$ã€‚<p>\n",
    "å°†$P_i,i\\in[1,N]$çš„$\\hat{K}$è®¾ç½®ä¸º0,å³$\\bar{K}$ã€‚<p>\n",
    "å°†ä¸¤è€…æ‹¼æ¥èµ·æ¥ï¼Œ$\\tilde{K}=\\text{Concat}(\\tilde{K}^T,\\bar{K}^T)^T$,åŒç†,$\\tilde{\\mathcal{V}}=\\mathrm{Concat}(\\tilde{\\mathcal{V}}^{\\top},\\bar{\\mathcal{V}}^{\\top})^{\\top}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tilde{\\mathcal{A}}=softmax\\left(\\tilde{\\mathcal{Q}}\\tilde{\\mathcal{K}}^\\top/\\sqrt{d}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def ipca(q, k, v, scale, unet_controller: Optional[UNetController] = None): # eg. q: [4,20,1024,64] k,v: [4,20,77,64] \n",
    "    # æ²¿ç€batchç»´åº¦ç»™æˆnegative promptå’Œpositive promptçš„qkv\n",
    "    # q:batch_size,num_heads,seq_len_q,head_dim\n",
    "    # k,v:batch_size,num_heads,seq_len_k,head_dim\n",
    "    q_neg, q_pos = torch.split(q, q.size(0) // 2, dim=0)\n",
    "    k_neg, k_pos = torch.split(k, k.size(0) // 2, dim=0)\n",
    "    v_neg, v_pos = torch.split(v, v.size(0) // 2, dim=0)\n",
    "\n",
    "    # 1. negative_attnï¼Œnegativeæ³¨æ„åŠ›è®¡ç®—\n",
    "    # å°†åä¸¤ä¸ªç»´åº¦è¿›è¡Œè½¬èŒä»¥ä¾¿è¿›è¡ŒçŸ©é˜µæƒ©ç½š\n",
    "    scores_neg = torch.matmul(q_neg, k_neg.transpose(-2, -1)) * scale\n",
    "    # scores_neg:..,..,seq_len_q,seq_len_k\n",
    "    attn_weights_neg = torch.softmax(scores_neg, dim=-1)\n",
    "    attn_output_neg = torch.matmul(attn_weights_neg, v_neg)\n",
    "\n",
    "    # 2. positive_attn (we do ipca only on positive branch)\n",
    "\n",
    "    # 2.1 ipca \n",
    "    # k_pos: [batch_size_pos, num_heads, seq_len_k, head_dim]\n",
    "    # k_pos.transpose(-2, -1): [batch_size_pos, num_heads, head_dim, seq_len_k]\n",
    "    # tuple(...): å°†æŒ‰å¤´åˆ†å‰²çš„Keyå¼ é‡å˜æˆä¸€ä¸ªå…ƒç»„ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ [batch_size_pos, head_dim, seq_len_k]\n",
    "    # torch.cat(..., dim=2):  [batch_size_pos, head_dim, num_heads * seq_len_k]\n",
    "    # unsequeeze(0):[1, batch_size_pos, head_dim, num_heads * seq_len_k]\n",
    "    # .repeat:[batch_size_pos, batch_size_pos, head_dim, num_heads * seq_len_k]\n",
    "\n",
    "    # æ‹¼æ¥åçš„K\n",
    "    k_plus = torch.cat(tuple(k_pos.transpose(-2, -1)), dim=2).unsqueeze(0).repeat(k_pos.size(0),1,1,1) # ğ¾+ = [ğ¾1 âŠ• ğ¾2 âŠ• . . . âŠ• ğ¾ğ‘ ]\n",
    "    # æ‹¼æ¥åçš„V\n",
    "    v_plus = torch.cat(tuple(v_pos), dim=1).unsqueeze(0).repeat(v_pos.size(0),1,1,1) # ğ‘‰+ = [ğ‘‰1 âŠ• ğ‘‰2 âŠ• . . . âŠ• ğ‘‰ğ‘ ]\n",
    "\n",
    "\n",
    "    # 2.2 apply mask\n",
    "    if unet_controller is not None:\n",
    "        scores_pos = torch.matmul(q_pos, k_plus) * scale\n",
    "\n",
    "        # 2.2.1 apply dropout mask\n",
    "        dropout_mask = gen_dropout_mask(scores_pos.shape, unet_controller, unet_controller.Ipca_dropout) # eg: [a,1024,154]   \n",
    "\n",
    "\n",
    "        # 2.2.3 apply embeds mask\n",
    "        if unet_controller.Use_embeds_mask:\n",
    "            apply_embeds_mask(unet_controller,dropout_mask, add_eot=False)\n",
    "\n",
    "        mask = dropout_mask\n",
    "\n",
    "        mask = mask.unsqueeze(1).repeat(1,scores_pos.size(1),1,1)\n",
    "        attn_weights_pos = torch.softmax(scores_pos + torch.log(mask), dim=-1)\n",
    "\n",
    "    else:\n",
    "        scores_pos = torch.matmul(q_pos, k_plus) * scale\n",
    "        attn_weights_pos = torch.softmax(scores_pos, dim=-1)\n",
    "\n",
    "\n",
    "    attn_output_pos = torch.matmul(attn_weights_pos, v_plus)\n",
    "    # 3. combine\n",
    "    attn_output = torch.cat((attn_output_neg, attn_output_pos), dim=0)\n",
    "\n",
    "    return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipca2(q, k, v, scale, unet_controller: Optional[UNetController] = None): # eg. q: [4,20,1024,64] k,v: [4,20,77,64] \n",
    "    if unet_controller.ipca_time_step != unet_controller.current_time_step:\n",
    "        unet_controller.ipca_time_step = unet_controller.current_time_step\n",
    "        unet_controller.ipca2_index = 0\n",
    "    else:\n",
    "        unet_controller.ipca2_index += 1\n",
    "\n",
    "    if unet_controller.Store_qkv is True:\n",
    "\n",
    "        key = f\"cross {unet_controller.current_time_step} {unet_controller.current_unet_position} {unet_controller.ipca2_index}\"\n",
    "        unet_controller.k_store[key] = k\n",
    "        unet_controller.v_store[key] = v\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "    else:\n",
    "        # batch > 1\n",
    "        if unet_controller.frame_prompt_express_list is not None:\n",
    "            batch_size = q.size(0) // 2\n",
    "            attn_output_list = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                q_i = q[[i, i + batch_size], :, :, :]\n",
    "                k_i = k[[i, i + batch_size], :, :, :]\n",
    "                v_i = v[[i, i + batch_size], :, :, :]\n",
    "\n",
    "                q_neg_i, q_pos_i = torch.split(q_i, q_i.size(0) // 2, dim=0)\n",
    "                k_neg_i, k_pos_i = torch.split(k_i, k_i.size(0) // 2, dim=0)\n",
    "                v_neg_i, v_pos_i = torch.split(v_i, v_i.size(0) // 2, dim=0)\n",
    "\n",
    "                key = f\"cross {unet_controller.current_time_step} {unet_controller.current_unet_position} {unet_controller.ipca2_index}\"\n",
    "                q_store = q_i\n",
    "                k_store = unet_controller.k_store[key]\n",
    "                v_store = unet_controller.v_store[key]\n",
    "\n",
    "                q_store_neg, q_store_pos = torch.split(q_store, q_store.size(0) // 2, dim=0)\n",
    "                k_store_neg, k_store_pos = torch.split(k_store, k_store.size(0) // 2, dim=0)\n",
    "                v_store_neg, v_store_pos = torch.split(v_store, v_store.size(0) // 2, dim=0)    \n",
    "\n",
    "                q_neg = torch.cat((q_neg_i, q_store_neg), dim=0)\n",
    "                q_pos = torch.cat((q_pos_i, q_store_pos), dim=0)\n",
    "                k_neg = torch.cat((k_neg_i, k_store_neg), dim=0)\n",
    "                k_pos = torch.cat((k_pos_i, k_store_pos), dim=0)\n",
    "                v_neg = torch.cat((v_neg_i, v_store_neg), dim=0)\n",
    "                v_pos = torch.cat((v_pos_i, v_store_pos), dim=0)\n",
    "\n",
    "                q_i = torch.cat((q_neg, q_pos), dim=0)\n",
    "                k_i = torch.cat((k_neg, k_pos), dim=0)\n",
    "                v_i = torch.cat((v_neg, v_pos), dim=0)\n",
    "\n",
    "                attn_output_i = ipca(q_i, k_i, v_i, scale, unet_controller)\n",
    "                attn_output_i = attn_output_i[[0, 2], :, :, :]\n",
    "                attn_output_list.append(attn_output_i)\n",
    "            \n",
    "            attn_output_ = torch.cat(attn_output_list, dim=0)\n",
    "            attn_output = torch.zeros(size=(q.size(0), attn_output_i.size(1), attn_output_i.size(2), attn_output_i.size(3)), device=q.device, dtype=q.dtype)\n",
    "            for i in range(batch_size):\n",
    "                attn_output[i] = attn_output_[i*2]\n",
    "            for i in range(batch_size):\n",
    "                attn_output[i + batch_size] = attn_output_[i*2 + 1]\n",
    "        # batch = 1\n",
    "        else:\n",
    "            q_neg, q_pos = torch.split(q, q.size(0) // 2, dim=0)\n",
    "            k_neg, k_pos = torch.split(k, k.size(0) // 2, dim=0)\n",
    "            v_neg, v_pos = torch.split(v, v.size(0) // 2, dim=0)\n",
    "\n",
    "            key = f\"cross {unet_controller.current_time_step} {unet_controller.current_unet_position} {unet_controller.ipca2_index}\"\n",
    "            q_store = q\n",
    "            k_store = unet_controller.k_store[key]\n",
    "            v_store = unet_controller.v_store[key]\n",
    "\n",
    "            q_store_neg, q_store_pos = torch.split(q_store, q_store.size(0) // 2, dim=0)\n",
    "            k_store_neg, k_store_pos = torch.split(k_store, k_store.size(0) // 2, dim=0)\n",
    "            v_store_neg, v_store_pos = torch.split(v_store, v_store.size(0) // 2, dim=0)    \n",
    "\n",
    "            q_neg = torch.cat((q_neg, q_store_neg), dim=0)\n",
    "            q_pos = torch.cat((q_pos, q_store_pos), dim=0)\n",
    "            k_neg = torch.cat((k_neg, k_store_neg), dim=0)\n",
    "            k_pos = torch.cat((k_pos, k_store_pos), dim=0)\n",
    "            v_neg = torch.cat((v_neg, v_store_neg), dim=0)\n",
    "            v_pos = torch.cat((v_pos, v_store_pos), dim=0)\n",
    "\n",
    "            q = torch.cat((q_neg, q_pos), dim=0)\n",
    "            k = torch.cat((k_neg, k_pos), dim=0)\n",
    "            v = torch.cat((v_neg, v_pos), dim=0)\n",
    "\n",
    "            attn_output = ipca(q, k, v, scale, unet_controller)\n",
    "            attn_output = attn_output[[0, 2], :, :, :]\n",
    "    \n",
    "    return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_embeds_mask(unet_controller: Optional[UNetController],dropout_mask, add_eot=False):   \n",
    "    id_prompt = unet_controller.id_prompt\n",
    "    prompt_tokens = prompt2tokens(unet_controller.tokenizer,unet_controller.prompts[0])\n",
    "    \n",
    "    words_tokens = prompt2tokens(unet_controller.tokenizer,id_prompt)\n",
    "    words_tokens = [word for word in words_tokens if word != '<|endoftext|>' and word != '<|startoftext|>']\n",
    "    index_of_words = find_sublist_index(prompt_tokens,words_tokens)    \n",
    "    index_list = [index+77 for index in range(index_of_words, index_of_words+len(words_tokens))]\n",
    "    if add_eot:\n",
    "        index_list.extend([index+77 for index, word in enumerate(prompt_tokens) if word == '<|endoftext|>'])\n",
    "\n",
    "    mask_indices = torch.arange(dropout_mask.size(-1), device=dropout_mask.device)\n",
    "    mask = (mask_indices >= 78) & (~torch.isin(mask_indices, torch.tensor(index_list, device=dropout_mask.device)))\n",
    "    dropout_mask[0, :, mask] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dropout_mask(out_shape, unet_controller: Optional[UNetController], drop_out):\n",
    "    gen_length = out_shape[3]\n",
    "    attn_map_side_length = out_shape[2]\n",
    "\n",
    "    batch_num = out_shape[0]\n",
    "    mask_list = []\n",
    "    \n",
    "    for prompt_index in range(batch_num):\n",
    "        start = prompt_index * int(gen_length / batch_num)\n",
    "        end = (prompt_index + 1) * int(gen_length / batch_num)\n",
    "    \n",
    "        mask = torch.bernoulli(torch.full((attn_map_side_length,gen_length), 1 - drop_out, dtype=unet_controller.torch_dtype, device=unet_controller.device))        \n",
    "        mask[:, start:end] = 1\n",
    "\n",
    "        mask_list.append(mask)\n",
    "\n",
    "    concatenated_mask = torch.stack(mask_list, dim=0)\n",
    "    return concatenated_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../../Image/overall_2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heavy_daily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
