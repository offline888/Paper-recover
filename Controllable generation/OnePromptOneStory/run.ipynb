{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt——ICLR 2025 Spotlight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决T2I的主体一致性问题，可以视为SuppressEOT的续作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仓库代码组件\n",
    "- Consistent Image Generation Code: `main.py`\n",
    "- Gradio Code: `app.py`\n",
    "- Benchmark Generation Code: `resource/gen_benchmark.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Consitency in Text Embedding\n",
    "- Multiprompt generation:$[P_0;P_i],i\\in[1,N]$\n",
    "    - $P_0$:identity prompt,$P_i$:i-th frame prompt\n",
    "    - 第1帧: $[P_{0};P_{1}]$ → “一只可爱的水彩画风格的小猫在花园里”\n",
    "\n",
    "    - 第2帧: $[P_{0};P_{2}]$ → “一只可爱的水彩画风格的小猫穿着超人斗篷”\n",
    "\n",
    "    - 第3帧: $[P_{0};P_{3}]$ → “一只可爱的水彩画风格的小猫戴着项圈和铃铛”\n",
    "\n",
    "    - 第4帧: $[P_{0};P_{4}]$ → “一只可爱的水彩画风格的小猫坐在篮子里”\n",
    "\n",
    "    - 第5帧: $[P_{0};P_{5}]$ → “一只可爱的水彩画风格的小猫穿着可爱的毛衣”\n",
    "- Singleprompt generation:$[P_0;P_1;P_2;...;P_N]$\n",
    "     - 单一提示 P: “一只可爱的水彩画风格的小猫在花园里，穿着超人斗篷，戴着项圈和铃铛，坐在篮子里，穿着可爱的毛衣”,然后，我们通过调整每个场景描述的权重来生成每个场景的图像。例如：\n",
    "\n",
    "    - 第1帧：通过增强“在花园里”的权重，生成“一只可爱的水彩画风格的小猫在花园里”的图像。\n",
    "\n",
    "    - 第2帧：通过增强“穿着超人斗篷”的权重，生成“一只可爱的水彩画风格的小猫穿着超人斗篷”的图像。\n",
    "\n",
    "    - 第3帧：通过增强“戴着项圈和铃铛”的权重，生成“一只可爱的水彩画风格的小猫戴着项圈和铃铛”的图像。\n",
    "\n",
    "    - 第4帧：通过增强“坐在篮子里”的权重，生成“一只可爱的水彩画风格的小猫坐在篮子里”的图像。\n",
    "\n",
    "    - 第5帧：通过增强“穿着可爱的毛衣”的权重，生成“一只可爱的水彩画风格的小猫穿着可爱的毛衣”的图像。\n",
    "-  multiprompt embedding:$\\mathcal{C} _i= \\tau _\\xi ( [ \\mathcal{P} _0; \\mathcal{P} _i] ) =[c^{SOT},c^{\\mathcal{P}_0},c^{\\mathcal{P}_i},c^{EOT}],(i=1,\\ldots,N)$\n",
    "- singleprompt embedding:$\\mathcal{C}=\\tau_\\xi([\\mathcal{P}_0;\\mathcal{P}_1;\\ldots;\\mathcal{P}_N])=[\\boldsymbol{c}^{SOT},\\boldsymbol{c}^{\\mathcal{P}_0},\\boldsymbol{c}^{\\mathcal{P}_1},\\ldots,\\boldsymbol{c}^{\\mathcal{P}_N},\\boldsymbol{c}^{EOT}].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../../Image/t-SNE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Consitency in Imgae Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Prompt Reweighting：使用一个缩放因子对特定的embeeding进行scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了**可视化图像之间的身份相似性**，使用Carvkit移除背景，使用DINO-v2提取视觉特征，然后将这些特征通过t-SNE投影到2D空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<../../Image/屏幕截图 2025-05-26 131702.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "- Prompt Consolidation(above)\n",
    "- Singular Value Reweighting(SVR)\n",
    "- Identity-Preserving Cross-Attention (IPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Reweighting\n",
    "延续SupressEOT的思路，进行改进<p>\n",
    "奇异值重新加权（Singular-Value Reweighting）的使用可以减少单提示生成中帧描述的混合\n",
    "- SVR+:$\\hat{\\sigma}=\\beta e^{\\alpha\\sigma}*\\sigma$\n",
    "- SVR-:$\\tilde{\\sigma}=\\beta^{\\prime}e^{-\\alpha^{\\prime}\\hat{\\sigma}}*\\hat{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swr_single_prompt_embeds(swr_words,prompt_embeds,prompt,tokenizer,alpha=1.0, beta=1.2, zero_eot=False):\n",
    "    # swr_words:Suppress words,想要弱化的词\n",
    "    # prompt:原始的完整文本提示字符串\n",
    "    # alpha,beta:punish_wight函数的参数\n",
    "    # zero_eot:决定如何处理EOT token的输入\n",
    "    punish_indices = []\n",
    "\n",
    "    prompt_tokens = prompt2tokens(tokenizer,prompt)\n",
    "    \n",
    "    words_tokens = prompt2tokens(tokenizer,swr_words)\n",
    "    words_tokens = [word for word in words_tokens if word != '<|endoftext|>' and word != '<|startoftext|>']\n",
    "    index_of_words = find_sublist_index(prompt_tokens,words_tokens)\n",
    "    \n",
    "    if index_of_words != -1:\n",
    "        punish_indices.extend([num for num in range(index_of_words, index_of_words+len(words_tokens))])\n",
    "    \n",
    "    if zero_eot:\n",
    "        # 找到token sequence中的所有EOT的索引\n",
    "        eot_indices = [index for index, word in enumerate(prompt_tokens) if word == '<|endoftext|>']\n",
    "        # 将EOT token索引对应的embedding乘0.9，进行削弱\n",
    "        prompt_embeds[eot_indices] *= 9e-1\n",
    "        pass\n",
    "    else:\n",
    "        punish_indices.extend([index for index, word in enumerate(prompt_tokens) if word == '<|endoftext|>'])\n",
    "\n",
    "    punish_indices = list(set(punish_indices))\n",
    "    # 从完整的 prompt_embeds 张量中，提取出所有索引在 punish_indices 列表中的 token 嵌入\n",
    "    # wo_batch：[num_indices_to_punish, embedding_dim]\n",
    "    wo_batch = prompt_embeds[punish_indices]\n",
    "    # ------------------------------------------------------------------\n",
    "    wo_batch = punish_wight(wo_batch.T.to(float), \n",
    "                            wo_batch.size(0), \n",
    "                            alpha=alpha, \n",
    "                            beta=beta, \n",
    "                            calc_similarity=False).T.to(prompt_embeds.dtype)\n",
    "    # ------------------------------------------------------------------\n",
    "    prompt_embeds[punish_indices] = wo_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将prompt转换为input_ids之后padding到max_length，然后decode为token sequence\n",
    "def prompt2tokens(tokenizer, prompt):\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids\n",
    "    tokens = []\n",
    "    for text_input_id in text_input_ids[0]:\n",
    "        token = tokenizer.decoder[text_input_id.item()]\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cdist\n",
    "def punish_wight(tensor, latent_size, alpha=1.0, beta=1.2, calc_similarity=False):\n",
    "    u, s, vh = torch.linalg.svd(tensor)\n",
    "    u = u[:,:latent_size]\n",
    "    zero_idx = int(latent_size * alpha)\n",
    "\n",
    "    if calc_similarity:\n",
    "        _s = s.clone()\n",
    "        _s *= torch.exp(-alpha*_s) * beta\n",
    "        _s[zero_idx:] = 0\n",
    "        _tensor = u @ torch.diag(_s) @ vh\n",
    "        dist = cdist(tensor[:,0].unsqueeze(0).cpu(), _tensor[:,0].unsqueeze(0).cpu(), metric='cosine')\n",
    "        print(f'The distance between the word embedding before and after the punishment: {dist}')\n",
    "    # ------------------------------------------------------------\n",
    "    s *= torch.exp(-alpha*s) * beta\n",
    "    # ------------------------------------------------------------\n",
    "    tensor = u @ torch.diag(s) @ vh\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity-Preserving Cross-Attention \n",
    "- 交叉注意力图：能够捕捉标记的特征信息，即与文本提示中的特定部分相关的语义信息<p>\n",
    "  自注意力：保留了图像的布局信息和形状细节，即与图像的整体结构和外观相关的视觉信息<p>\n",
    "(From 《Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing (CVPR 2024)》)\n",
    "- https://github.com/alibaba/EasyNLP/tree/master/diffusion/FreePromptEditing\n",
    "- SVR在单提示生成中减少不同帧描述之间的混合，但它可能对单提示内的上下文一致性产生负面影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR之后,我们得到了更新后的$\\hat{C}$;在去噪过程中，在Cross-attention过程得到$\\hat{Q},\\hat{K},\\hat{V}$。<p>\n",
    "将$P_i,i\\in[1,N]$的$\\hat{K}$设置为0,即$\\bar{K}$。<p>\n",
    "将两者拼接起来，$\\tilde{K}=\\text{Concat}(\\tilde{K}^T,\\bar{K}^T)^T$,同理,$\\tilde{\\mathcal{V}}=\\mathrm{Concat}(\\tilde{\\mathcal{V}}^{\\top},\\bar{\\mathcal{V}}^{\\top})^{\\top}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tilde{\\mathcal{A}}=softmax\\left(\\tilde{\\mathcal{Q}}\\tilde{\\mathcal{K}}^\\top/\\sqrt{d}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def ipca(q, k, v, scale, unet_controller: Optional[UNetController] = None): # eg. q: [4,20,1024,64] k,v: [4,20,77,64] \n",
    "    # 沿着batch维度给成negative prompt和positive prompt的qkv\n",
    "    # q:batch_size,num_heads,seq_len_q,head_dim\n",
    "    # k,v:batch_size,num_heads,seq_len_k,head_dim\n",
    "    q_neg, q_pos = torch.split(q, q.size(0) // 2, dim=0)\n",
    "    k_neg, k_pos = torch.split(k, k.size(0) // 2, dim=0)\n",
    "    v_neg, v_pos = torch.split(v, v.size(0) // 2, dim=0)\n",
    "\n",
    "    # 1. negative_attn，negative注意力计算\n",
    "    # 将后两个维度进行转职以便进行矩阵惩罚\n",
    "    scores_neg = torch.matmul(q_neg, k_neg.transpose(-2, -1)) * scale\n",
    "    # scores_neg:..,..,seq_len_q,seq_len_k\n",
    "    attn_weights_neg = torch.softmax(scores_neg, dim=-1)\n",
    "    attn_output_neg = torch.matmul(attn_weights_neg, v_neg)\n",
    "\n",
    "    # 2. positive_attn (we do ipca only on positive branch)\n",
    "\n",
    "    # 2.1 ipca \n",
    "    # k_pos: [batch_size_pos, num_heads, seq_len_k, head_dim]\n",
    "    # k_pos.transpose(-2, -1): [batch_size_pos, num_heads, head_dim, seq_len_k]\n",
    "    # tuple(...): 将按头分割的Key张量变成一个元组，每个元素是 [batch_size_pos, head_dim, seq_len_k]\n",
    "    # torch.cat(..., dim=2):  [batch_size_pos, head_dim, num_heads * seq_len_k]\n",
    "    # unsequeeze(0):[1, batch_size_pos, head_dim, num_heads * seq_len_k]\n",
    "    # .repeat:[batch_size_pos, batch_size_pos, head_dim, num_heads * seq_len_k]\n",
    "\n",
    "    # 拼接后的K\n",
    "    k_plus = torch.cat(tuple(k_pos.transpose(-2, -1)), dim=2).unsqueeze(0).repeat(k_pos.size(0),1,1,1) # 𝐾+ = [𝐾1 ⊕ 𝐾2 ⊕ . . . ⊕ 𝐾𝑁 ]\n",
    "    # 拼接后的V\n",
    "    v_plus = torch.cat(tuple(v_pos), dim=1).unsqueeze(0).repeat(v_pos.size(0),1,1,1) # 𝑉+ = [𝑉1 ⊕ 𝑉2 ⊕ . . . ⊕ 𝑉𝑁 ]\n",
    "\n",
    "\n",
    "    # 2.2 apply mask\n",
    "    if unet_controller is not None:\n",
    "        scores_pos = torch.matmul(q_pos, k_plus) * scale\n",
    "\n",
    "        # 2.2.1 apply dropout mask\n",
    "        dropout_mask = gen_dropout_mask(scores_pos.shape, unet_controller, unet_controller.Ipca_dropout) # eg: [a,1024,154]   \n",
    "\n",
    "\n",
    "        # 2.2.3 apply embeds mask\n",
    "        if unet_controller.Use_embeds_mask:\n",
    "            apply_embeds_mask(unet_controller,dropout_mask, add_eot=False)\n",
    "\n",
    "        mask = dropout_mask\n",
    "\n",
    "        mask = mask.unsqueeze(1).repeat(1,scores_pos.size(1),1,1)\n",
    "        attn_weights_pos = torch.softmax(scores_pos + torch.log(mask), dim=-1)\n",
    "\n",
    "    else:\n",
    "        scores_pos = torch.matmul(q_pos, k_plus) * scale\n",
    "        attn_weights_pos = torch.softmax(scores_pos, dim=-1)\n",
    "\n",
    "\n",
    "    attn_output_pos = torch.matmul(attn_weights_pos, v_plus)\n",
    "    # 3. combine\n",
    "    attn_output = torch.cat((attn_output_neg, attn_output_pos), dim=0)\n",
    "\n",
    "    return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipca2(q, k, v, scale, unet_controller: Optional[UNetController] = None): # eg. q: [4,20,1024,64] k,v: [4,20,77,64] \n",
    "    if unet_controller.ipca_time_step != unet_controller.current_time_step:\n",
    "        unet_controller.ipca_time_step = unet_controller.current_time_step\n",
    "        unet_controller.ipca2_index = 0\n",
    "    else:\n",
    "        unet_controller.ipca2_index += 1\n",
    "\n",
    "    if unet_controller.Store_qkv is True:\n",
    "\n",
    "        key = f\"cross {unet_controller.current_time_step} {unet_controller.current_unet_position} {unet_controller.ipca2_index}\"\n",
    "        unet_controller.k_store[key] = k\n",
    "        unet_controller.v_store[key] = v\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "    else:\n",
    "        # batch > 1\n",
    "        if unet_controller.frame_prompt_express_list is not None:\n",
    "            batch_size = q.size(0) // 2\n",
    "            attn_output_list = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                q_i = q[[i, i + batch_size], :, :, :]\n",
    "                k_i = k[[i, i + batch_size], :, :, :]\n",
    "                v_i = v[[i, i + batch_size], :, :, :]\n",
    "\n",
    "                q_neg_i, q_pos_i = torch.split(q_i, q_i.size(0) // 2, dim=0)\n",
    "                k_neg_i, k_pos_i = torch.split(k_i, k_i.size(0) // 2, dim=0)\n",
    "                v_neg_i, v_pos_i = torch.split(v_i, v_i.size(0) // 2, dim=0)\n",
    "\n",
    "                key = f\"cross {unet_controller.current_time_step} {unet_controller.current_unet_position} {unet_controller.ipca2_index}\"\n",
    "                q_store = q_i\n",
    "                k_store = unet_controller.k_store[key]\n",
    "                v_store = unet_controller.v_store[key]\n",
    "\n",
    "                q_store_neg, q_store_pos = torch.split(q_store, q_store.size(0) // 2, dim=0)\n",
    "                k_store_neg, k_store_pos = torch.split(k_store, k_store.size(0) // 2, dim=0)\n",
    "                v_store_neg, v_store_pos = torch.split(v_store, v_store.size(0) // 2, dim=0)    \n",
    "\n",
    "                q_neg = torch.cat((q_neg_i, q_store_neg), dim=0)\n",
    "                q_pos = torch.cat((q_pos_i, q_store_pos), dim=0)\n",
    "                k_neg = torch.cat((k_neg_i, k_store_neg), dim=0)\n",
    "                k_pos = torch.cat((k_pos_i, k_store_pos), dim=0)\n",
    "                v_neg = torch.cat((v_neg_i, v_store_neg), dim=0)\n",
    "                v_pos = torch.cat((v_pos_i, v_store_pos), dim=0)\n",
    "\n",
    "                q_i = torch.cat((q_neg, q_pos), dim=0)\n",
    "                k_i = torch.cat((k_neg, k_pos), dim=0)\n",
    "                v_i = torch.cat((v_neg, v_pos), dim=0)\n",
    "\n",
    "                attn_output_i = ipca(q_i, k_i, v_i, scale, unet_controller)\n",
    "                attn_output_i = attn_output_i[[0, 2], :, :, :]\n",
    "                attn_output_list.append(attn_output_i)\n",
    "            \n",
    "            attn_output_ = torch.cat(attn_output_list, dim=0)\n",
    "            attn_output = torch.zeros(size=(q.size(0), attn_output_i.size(1), attn_output_i.size(2), attn_output_i.size(3)), device=q.device, dtype=q.dtype)\n",
    "            for i in range(batch_size):\n",
    "                attn_output[i] = attn_output_[i*2]\n",
    "            for i in range(batch_size):\n",
    "                attn_output[i + batch_size] = attn_output_[i*2 + 1]\n",
    "        # batch = 1\n",
    "        else:\n",
    "            q_neg, q_pos = torch.split(q, q.size(0) // 2, dim=0)\n",
    "            k_neg, k_pos = torch.split(k, k.size(0) // 2, dim=0)\n",
    "            v_neg, v_pos = torch.split(v, v.size(0) // 2, dim=0)\n",
    "\n",
    "            key = f\"cross {unet_controller.current_time_step} {unet_controller.current_unet_position} {unet_controller.ipca2_index}\"\n",
    "            q_store = q\n",
    "            k_store = unet_controller.k_store[key]\n",
    "            v_store = unet_controller.v_store[key]\n",
    "\n",
    "            q_store_neg, q_store_pos = torch.split(q_store, q_store.size(0) // 2, dim=0)\n",
    "            k_store_neg, k_store_pos = torch.split(k_store, k_store.size(0) // 2, dim=0)\n",
    "            v_store_neg, v_store_pos = torch.split(v_store, v_store.size(0) // 2, dim=0)    \n",
    "\n",
    "            q_neg = torch.cat((q_neg, q_store_neg), dim=0)\n",
    "            q_pos = torch.cat((q_pos, q_store_pos), dim=0)\n",
    "            k_neg = torch.cat((k_neg, k_store_neg), dim=0)\n",
    "            k_pos = torch.cat((k_pos, k_store_pos), dim=0)\n",
    "            v_neg = torch.cat((v_neg, v_store_neg), dim=0)\n",
    "            v_pos = torch.cat((v_pos, v_store_pos), dim=0)\n",
    "\n",
    "            q = torch.cat((q_neg, q_pos), dim=0)\n",
    "            k = torch.cat((k_neg, k_pos), dim=0)\n",
    "            v = torch.cat((v_neg, v_pos), dim=0)\n",
    "\n",
    "            attn_output = ipca(q, k, v, scale, unet_controller)\n",
    "            attn_output = attn_output[[0, 2], :, :, :]\n",
    "    \n",
    "    return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_embeds_mask(unet_controller: Optional[UNetController],dropout_mask, add_eot=False):   \n",
    "    id_prompt = unet_controller.id_prompt\n",
    "    prompt_tokens = prompt2tokens(unet_controller.tokenizer,unet_controller.prompts[0])\n",
    "    \n",
    "    words_tokens = prompt2tokens(unet_controller.tokenizer,id_prompt)\n",
    "    words_tokens = [word for word in words_tokens if word != '<|endoftext|>' and word != '<|startoftext|>']\n",
    "    index_of_words = find_sublist_index(prompt_tokens,words_tokens)    \n",
    "    index_list = [index+77 for index in range(index_of_words, index_of_words+len(words_tokens))]\n",
    "    if add_eot:\n",
    "        index_list.extend([index+77 for index, word in enumerate(prompt_tokens) if word == '<|endoftext|>'])\n",
    "\n",
    "    mask_indices = torch.arange(dropout_mask.size(-1), device=dropout_mask.device)\n",
    "    mask = (mask_indices >= 78) & (~torch.isin(mask_indices, torch.tensor(index_list, device=dropout_mask.device)))\n",
    "    dropout_mask[0, :, mask] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dropout_mask(out_shape, unet_controller: Optional[UNetController], drop_out):\n",
    "    gen_length = out_shape[3]\n",
    "    attn_map_side_length = out_shape[2]\n",
    "\n",
    "    batch_num = out_shape[0]\n",
    "    mask_list = []\n",
    "    \n",
    "    for prompt_index in range(batch_num):\n",
    "        start = prompt_index * int(gen_length / batch_num)\n",
    "        end = (prompt_index + 1) * int(gen_length / batch_num)\n",
    "    \n",
    "        mask = torch.bernoulli(torch.full((attn_map_side_length,gen_length), 1 - drop_out, dtype=unet_controller.torch_dtype, device=unet_controller.device))        \n",
    "        mask[:, start:end] = 1\n",
    "\n",
    "        mask_list.append(mask)\n",
    "\n",
    "    concatenated_mask = torch.stack(mask_list, dim=0)\n",
    "    return concatenated_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../../Image/overall_2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heavy_daily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
